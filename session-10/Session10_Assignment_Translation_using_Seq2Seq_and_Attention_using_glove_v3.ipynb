{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Session10_Assignment_Translation_using_Seq2Seq_and_Attention_using_glove_v3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "vxCla6HFCtRH"
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4Sm1RVyDOVE"
      },
      "source": [
        "\n",
        "NLP From Scratch: Translation with a Sequence to Sequence Network and Attention\n",
        "*******************************************************************************\n",
        "**Author**: `Sean Robertson <https://github.com/spro/practical-pytorch>`_\n",
        "[PyTorch Source](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html)\n",
        "\n",
        "This is the third and final tutorial on doing \"NLP From Scratch\", where we\n",
        "write our own classes and functions to preprocess the data to do our NLP\n",
        "modeling tasks. We hope after you complete this tutorial that you'll proceed to\n",
        "learn how `torchtext` can handle much of this preprocessing for you in the\n",
        "three tutorials immediately following this one.\n",
        "\n",
        "In this project we will be teaching a neural network to translate from\n",
        "French to English.\n",
        "\n",
        "::\n",
        "\n",
        "    [KEY: > input, = target, < output]\n",
        "\n",
        "    > il est en train de peindre un tableau .\n",
        "    = he is painting a picture .\n",
        "    < he is painting a picture .\n",
        "\n",
        "    > pourquoi ne pas essayer ce vin delicieux ?\n",
        "    = why not try that delicious wine ?\n",
        "    < why not try that delicious wine ?\n",
        "\n",
        "    > elle n est pas poete mais romanciere .\n",
        "    = she is not a poet but a novelist .\n",
        "    < she not not a poet but a novelist .\n",
        "\n",
        "    > vous etes trop maigre .\n",
        "    = you re too skinny .\n",
        "    < you re all alone .\n",
        "\n",
        "... to varying degrees of success.\n",
        "\n",
        "This is made possible by the simple but powerful idea of the [sequence to sequence network](https://arxiv.org/abs/1409.3215), in which two\n",
        "recurrent neural networks work together to transform one sequence to\n",
        "another. An encoder network condenses an input sequence into a vector,\n",
        "and a decoder network unfolds that vector into a new sequence.\n",
        "\n",
        ".. figure:: /_static/img/seq-seq-images/seq2seq.png\n",
        "   :alt:\n",
        "\n",
        "To improve upon this model we'll use an [attention\n",
        "mechanism](https://arxiv.org/abs/1409.0473), which lets the decoder\n",
        "learn to focus over a specific range of the input sequence.\n",
        "\n",
        "**Recommended Reading:**\n",
        "\n",
        "I assume you have at least installed PyTorch, know Python, and\n",
        "understand Tensors:\n",
        "\n",
        "-  https://pytorch.org/ For installation instructions\n",
        "-  [Deep Learning with PyTorch](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html): A 60 Minute Blitz to get started with PyTorch in general\n",
        "-  [Learning PyTorch with Examples](https://pytorch.org/tutorials/beginner/pytorch_with_examples.html) for a wide and deep overview\n",
        "\n",
        "\n",
        "It would also be useful to know about Sequence to Sequence networks and\n",
        "how they work:\n",
        "\n",
        "-  [Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
        "-  [Sequence to Sequence Learning with Neural Networks](https://arxiv.org/abs/1409.3215)\n",
        "-  [Neural Machine Translation by Jointly Learning to Align and Translate](https://arxiv.org/abs/1409.0473)\n",
        "-  [A Neural Conversational Model](https://arxiv.org/abs/1506.05869)\n",
        "\n",
        "\n",
        "\n",
        "![Image](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "**Requirements**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32WwVptLgdFX",
        "outputId": "becf4817-5f34-4de9-a780-f88e48244e16"
      },
      "source": [
        "#bolz not available, install it \n",
        "!pip install bcolz\n",
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "import bcolz  # to process the data from Glove File \n",
        "import pickle # to dump and load pretrained glove vectors \n",
        "import copy   # to make deepcopy of python lists and dictionaries\n",
        "import operator\n",
        "import numpy as np\n",
        "from pandas import DataFrame # to visualize the glove word embeddings in form of DataFrame\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting bcolz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\r\u001b[K     |▎                               | 10kB 21.5MB/s eta 0:00:01\r\u001b[K     |▌                               | 20kB 27.4MB/s eta 0:00:01\r\u001b[K     |▊                               | 30kB 21.0MB/s eta 0:00:01\r\u001b[K     |█                               | 40kB 17.9MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51kB 15.4MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61kB 13.4MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71kB 14.9MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81kB 15.2MB/s eta 0:00:01\r\u001b[K     |██                              | 92kB 12.8MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102kB 13.7MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112kB 13.7MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122kB 13.7MB/s eta 0:00:01\r\u001b[K     |███                             | 133kB 13.7MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143kB 13.7MB/s eta 0:00:01\r\u001b[K     |███▍                            | 153kB 13.7MB/s eta 0:00:01\r\u001b[K     |███▋                            | 163kB 13.7MB/s eta 0:00:01\r\u001b[K     |███▉                            | 174kB 13.7MB/s eta 0:00:01\r\u001b[K     |████                            | 184kB 13.7MB/s eta 0:00:01\r\u001b[K     |████▎                           | 194kB 13.7MB/s eta 0:00:01\r\u001b[K     |████▌                           | 204kB 13.7MB/s eta 0:00:01\r\u001b[K     |████▊                           | 215kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████                           | 225kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████▏                          | 235kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 245kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 256kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 266kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████                          | 276kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 286kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 296kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 307kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████                         | 317kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 327kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 337kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 348kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 358kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 368kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 378kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 389kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 399kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████                       | 409kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 419kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 430kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 440kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 450kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████                      | 460kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 471kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 481kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 491kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████                     | 501kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 512kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 522kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 532kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 542kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 552kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 563kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 573kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 583kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 593kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 604kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 614kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████▊                  | 624kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 634kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 645kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 655kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 665kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 675kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 686kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 696kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 706kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 716kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████                | 727kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████▏               | 737kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████▍               | 747kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 757kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 768kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 778kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 788kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 798kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 808kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 819kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 829kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 839kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 849kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 860kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 870kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 880kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 890kB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 901kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 911kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▏           | 921kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▍           | 931kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 942kB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 952kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 962kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 972kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 983kB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 993kB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.0MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 1.0MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.0MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.0MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 1.1MB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▎     | 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.3MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.3MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.3MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3MB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3MB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3MB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3MB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.4MB 13.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.4MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.4MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.4MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.4MB 13.7MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.4MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.4MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.4MB 13.7MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 13.7MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.5MB 13.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from bcolz) (1.19.5)\n",
            "Building wheels for collected packages: bcolz\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-1.2.1-cp37-cp37m-linux_x86_64.whl size=2650789 sha256=db5e105914f0b92ba280b2df2f79b6e30764a4dbb2dfbc175e1e7649379f6373\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "Successfully built bcolz\n",
            "Installing collected packages: bcolz\n",
            "Successfully installed bcolz-1.2.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W08GAJzDtozn",
        "outputId": "b117c6db-d300-4621-ab64-08abf2f11988"
      },
      "source": [
        "!wget https://download.pytorch.org/tutorial/data.zip\n",
        "\n",
        "!unzip data.zip"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-17 18:05:59--  https://download.pytorch.org/tutorial/data.zip\n",
            "Resolving download.pytorch.org (download.pytorch.org)... 13.227.211.92, 13.227.211.25, 13.227.211.45, ...\n",
            "Connecting to download.pytorch.org (download.pytorch.org)|13.227.211.92|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2882130 (2.7M) [application/zip]\n",
            "Saving to: ‘data.zip’\n",
            "\n",
            "data.zip            100%[===================>]   2.75M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2021-07-17 18:06:00 (43.5 MB/s) - ‘data.zip’ saved [2882130/2882130]\n",
            "\n",
            "Archive:  data.zip\n",
            "   creating: data/\n",
            "  inflating: data/eng-fra.txt        \n",
            "   creating: data/names/\n",
            "  inflating: data/names/Arabic.txt   \n",
            "  inflating: data/names/Chinese.txt  \n",
            "  inflating: data/names/Czech.txt    \n",
            "  inflating: data/names/Dutch.txt    \n",
            "  inflating: data/names/English.txt  \n",
            "  inflating: data/names/French.txt   \n",
            "  inflating: data/names/German.txt   \n",
            "  inflating: data/names/Greek.txt    \n",
            "  inflating: data/names/Irish.txt    \n",
            "  inflating: data/names/Italian.txt  \n",
            "  inflating: data/names/Japanese.txt  \n",
            "  inflating: data/names/Korean.txt   \n",
            "  inflating: data/names/Polish.txt   \n",
            "  inflating: data/names/Portuguese.txt  \n",
            "  inflating: data/names/Russian.txt  \n",
            "  inflating: data/names/Scottish.txt  \n",
            "  inflating: data/names/Spanish.txt  \n",
            "  inflating: data/names/Vietnamese.txt  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pwUHzwGJg8z_",
        "outputId": "27b34351-2843-4ed2-ba6f-daaef281010b"
      },
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip glove.6B.zip\n",
        "!ls -lat"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-07-17 18:06:04--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2021-07-17 18:06:04--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2021-07-17 18:06:05--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.04MB/s    in 2m 42s  \n",
            "\n",
            "2021-07-17 18:08:47 (5.09 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n",
            "Archive:  glove.6B.zip\n",
            "  inflating: glove.6B.50d.txt        \n",
            "  inflating: glove.6B.100d.txt       \n",
            "  inflating: glove.6B.200d.txt       \n",
            "  inflating: glove.6B.300d.txt       \n",
            "total 3041968\n",
            "drwxr-xr-x 1 root root       4096 Jul 17 18:08 .\n",
            "drwxr-xr-x 1 root root       4096 Jul 17 18:00 ..\n",
            "drwxr-xr-x 1 root root       4096 Jul 15 13:38 sample_data\n",
            "drwxr-xr-x 4 root root       4096 Jul 15 13:37 .config\n",
            "-rw-r--r-- 1 root root    2882130 Mar 15  2017 data.zip\n",
            "drwxr-xr-x 3 root root       4096 Mar 12  2017 data\n",
            "-rw-r--r-- 1 root root  862182613 Oct 25  2015 glove.6B.zip\n",
            "-rw-rw-r-- 1 root root 1037962819 Aug 27  2014 glove.6B.300d.txt\n",
            "-rw-rw-r-- 1 root root  171350079 Aug  4  2014 glove.6B.50d.txt\n",
            "-rw-rw-r-- 1 root root  693432828 Aug  4  2014 glove.6B.200d.txt\n",
            "-rw-rw-r-- 1 root root  347116733 Aug  4  2014 glove.6B.100d.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GT-nJ-gBdul-"
      },
      "source": [
        "# from torchtext.vocab import GloVe\n",
        "# embedding_glove = GloVe(name='6B', dim=200)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "reHT84HOg82r"
      },
      "source": [
        "words_list = []\n",
        "index = 0\n",
        "word2index = {}\n",
        "vectors = bcolz.carray(np.zeros(1), mode='w')\n",
        "with open(f'/content/glove.6B.300d.txt', 'rb') as f:\n",
        "    for l in f:\n",
        "        line = l.decode().split()\n",
        "        word = line[0] \n",
        "        words_list.append(word)\n",
        "        word2index[word] = index\n",
        "        index += 1\n",
        "        vect = np.array(line[1:]).astype(np.float)\n",
        "        vectors.append(vect)\n",
        "vectors = bcolz.carray(vectors[1:].reshape((400000, 300)), mode='w')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TNEeL0LKbRQJ",
        "outputId": "ecedc2cf-cafe-4258-e3b2-c13601a09e45"
      },
      "source": [
        "print(len(vectors)),print(len(words_list)),print(len(word2index))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000\n",
            "400000\n",
            "400000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxkGa31EZhzq"
      },
      "source": [
        "### Re-arraging the indexes of sos and eos to begining and ending of the word vectors"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7v5fLujTZXzM"
      },
      "source": [
        "sos_index = word2index['sos']\n",
        "eos_index = word2index['eos']\n",
        "sos_swap_word = words_list[0]\n",
        "eos_swap_word = words_list[1]\n",
        "words_list[0], words_list[sos_index] = words_list[sos_index], words_list[0]\n",
        "words_list[1], words_list[eos_index] = words_list[eos_index], words_list[1]\n",
        "word2index[sos_swap_word], word2index['sos'] = word2index['sos'], word2index[sos_swap_word]\n",
        "word2index[eos_swap_word], word2index['eos'] = word2index['eos'], word2index[eos_swap_word]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyO3LJiKj9j5"
      },
      "source": [
        "glove = {w: vectors[word2index[w]] for w in words_list}\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vu7G0W0jqjdz",
        "outputId": "414c60b8-05c4-47a5-b6b4-c2807e0dba7a"
      },
      "source": [
        "print(len(vectors)),print(len(words_list)),print(len(word2index)),print(len(glove))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "400000\n",
            "400000\n",
            "400000\n",
            "400000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None, None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FN75BV8-qqdB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 412
        },
        "outputId": "d6784e2f-22bc-469e-8886-bf8b74bd604a"
      },
      "source": [
        "glove_dframe = DataFrame(vectors, columns=range(1,301), index=words_list)\n",
        "glove_dframe[100:110]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>261</th>\n",
              "      <th>262</th>\n",
              "      <th>263</th>\n",
              "      <th>264</th>\n",
              "      <th>265</th>\n",
              "      <th>266</th>\n",
              "      <th>267</th>\n",
              "      <th>268</th>\n",
              "      <th>269</th>\n",
              "      <th>270</th>\n",
              "      <th>271</th>\n",
              "      <th>272</th>\n",
              "      <th>273</th>\n",
              "      <th>274</th>\n",
              "      <th>275</th>\n",
              "      <th>276</th>\n",
              "      <th>277</th>\n",
              "      <th>278</th>\n",
              "      <th>279</th>\n",
              "      <th>280</th>\n",
              "      <th>281</th>\n",
              "      <th>282</th>\n",
              "      <th>283</th>\n",
              "      <th>284</th>\n",
              "      <th>285</th>\n",
              "      <th>286</th>\n",
              "      <th>287</th>\n",
              "      <th>288</th>\n",
              "      <th>289</th>\n",
              "      <th>290</th>\n",
              "      <th>291</th>\n",
              "      <th>292</th>\n",
              "      <th>293</th>\n",
              "      <th>294</th>\n",
              "      <th>295</th>\n",
              "      <th>296</th>\n",
              "      <th>297</th>\n",
              "      <th>298</th>\n",
              "      <th>299</th>\n",
              "      <th>300</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>so</th>\n",
              "      <td>-0.245610</td>\n",
              "      <td>0.068010</td>\n",
              "      <td>0.182540</td>\n",
              "      <td>-0.295510</td>\n",
              "      <td>0.018007</td>\n",
              "      <td>0.168110</td>\n",
              "      <td>0.191090</td>\n",
              "      <td>0.063650</td>\n",
              "      <td>0.212430</td>\n",
              "      <td>-2.1165</td>\n",
              "      <td>0.378640</td>\n",
              "      <td>-0.109740</td>\n",
              "      <td>-0.203400</td>\n",
              "      <td>0.085525</td>\n",
              "      <td>-0.321460</td>\n",
              "      <td>0.048442</td>\n",
              "      <td>-0.278470</td>\n",
              "      <td>-0.278830</td>\n",
              "      <td>0.128500</td>\n",
              "      <td>-0.10495</td>\n",
              "      <td>0.121340</td>\n",
              "      <td>0.459830</td>\n",
              "      <td>0.12976</td>\n",
              "      <td>0.030153</td>\n",
              "      <td>-0.330200</td>\n",
              "      <td>0.234990</td>\n",
              "      <td>0.151160</td>\n",
              "      <td>-0.25010</td>\n",
              "      <td>-0.121350</td>\n",
              "      <td>0.406420</td>\n",
              "      <td>0.052263</td>\n",
              "      <td>0.320160</td>\n",
              "      <td>-0.422180</td>\n",
              "      <td>-0.151860</td>\n",
              "      <td>-1.05650</td>\n",
              "      <td>0.305030</td>\n",
              "      <td>-0.001142</td>\n",
              "      <td>-0.090414</td>\n",
              "      <td>-0.075197</td>\n",
              "      <td>-0.079242</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.206720</td>\n",
              "      <td>-0.269500</td>\n",
              "      <td>-0.036829</td>\n",
              "      <td>0.080812</td>\n",
              "      <td>0.038849</td>\n",
              "      <td>0.161040</td>\n",
              "      <td>-0.114710</td>\n",
              "      <td>-0.073495</td>\n",
              "      <td>0.296550</td>\n",
              "      <td>0.180300</td>\n",
              "      <td>0.086003</td>\n",
              "      <td>0.025727</td>\n",
              "      <td>0.080528</td>\n",
              "      <td>0.214770</td>\n",
              "      <td>0.046840</td>\n",
              "      <td>0.301340</td>\n",
              "      <td>-2.1721</td>\n",
              "      <td>-0.137610</td>\n",
              "      <td>0.18022</td>\n",
              "      <td>0.18490</td>\n",
              "      <td>-0.207130</td>\n",
              "      <td>0.122920</td>\n",
              "      <td>0.204260</td>\n",
              "      <td>-0.081932</td>\n",
              "      <td>-0.232470</td>\n",
              "      <td>0.239880</td>\n",
              "      <td>0.153210</td>\n",
              "      <td>0.187450</td>\n",
              "      <td>-0.401180</td>\n",
              "      <td>-0.270270</td>\n",
              "      <td>0.095983</td>\n",
              "      <td>-0.347390</td>\n",
              "      <td>0.032640</td>\n",
              "      <td>0.047579</td>\n",
              "      <td>0.222420</td>\n",
              "      <td>-0.231400</td>\n",
              "      <td>0.114310</td>\n",
              "      <td>-0.669140</td>\n",
              "      <td>-0.034753</td>\n",
              "      <td>0.097869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>them</th>\n",
              "      <td>-0.205880</td>\n",
              "      <td>0.192570</td>\n",
              "      <td>-0.118270</td>\n",
              "      <td>-0.157120</td>\n",
              "      <td>-0.380160</td>\n",
              "      <td>0.059840</td>\n",
              "      <td>-0.032788</td>\n",
              "      <td>0.174650</td>\n",
              "      <td>0.403970</td>\n",
              "      <td>-1.8908</td>\n",
              "      <td>0.106380</td>\n",
              "      <td>-0.364760</td>\n",
              "      <td>0.054180</td>\n",
              "      <td>-0.027433</td>\n",
              "      <td>-0.188360</td>\n",
              "      <td>0.135500</td>\n",
              "      <td>-0.239940</td>\n",
              "      <td>-0.071888</td>\n",
              "      <td>0.363540</td>\n",
              "      <td>-0.12846</td>\n",
              "      <td>-0.001460</td>\n",
              "      <td>0.061712</td>\n",
              "      <td>0.58014</td>\n",
              "      <td>-0.038355</td>\n",
              "      <td>-0.273380</td>\n",
              "      <td>-0.289910</td>\n",
              "      <td>-0.332860</td>\n",
              "      <td>0.55739</td>\n",
              "      <td>0.145240</td>\n",
              "      <td>0.015591</td>\n",
              "      <td>-0.186130</td>\n",
              "      <td>0.104400</td>\n",
              "      <td>0.024976</td>\n",
              "      <td>-0.410220</td>\n",
              "      <td>-1.01080</td>\n",
              "      <td>-0.072071</td>\n",
              "      <td>-0.016702</td>\n",
              "      <td>0.288300</td>\n",
              "      <td>0.126030</td>\n",
              "      <td>-0.448160</td>\n",
              "      <td>...</td>\n",
              "      <td>0.144120</td>\n",
              "      <td>-0.644540</td>\n",
              "      <td>0.232860</td>\n",
              "      <td>0.021366</td>\n",
              "      <td>0.177000</td>\n",
              "      <td>0.148880</td>\n",
              "      <td>-0.330530</td>\n",
              "      <td>-0.180220</td>\n",
              "      <td>-0.312490</td>\n",
              "      <td>-0.168350</td>\n",
              "      <td>0.198440</td>\n",
              "      <td>0.021983</td>\n",
              "      <td>0.167490</td>\n",
              "      <td>0.601300</td>\n",
              "      <td>0.202740</td>\n",
              "      <td>0.001545</td>\n",
              "      <td>-2.7504</td>\n",
              "      <td>-0.190600</td>\n",
              "      <td>0.15862</td>\n",
              "      <td>0.10331</td>\n",
              "      <td>-0.278670</td>\n",
              "      <td>0.444220</td>\n",
              "      <td>0.092754</td>\n",
              "      <td>0.235970</td>\n",
              "      <td>-0.070352</td>\n",
              "      <td>0.438410</td>\n",
              "      <td>0.112300</td>\n",
              "      <td>0.355730</td>\n",
              "      <td>0.184230</td>\n",
              "      <td>-0.158480</td>\n",
              "      <td>-0.057513</td>\n",
              "      <td>-0.650600</td>\n",
              "      <td>0.028065</td>\n",
              "      <td>-0.135320</td>\n",
              "      <td>0.115990</td>\n",
              "      <td>-0.244490</td>\n",
              "      <td>0.048469</td>\n",
              "      <td>-0.328680</td>\n",
              "      <td>-0.169600</td>\n",
              "      <td>-0.129650</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>what</th>\n",
              "      <td>-0.200170</td>\n",
              "      <td>0.143020</td>\n",
              "      <td>0.052055</td>\n",
              "      <td>-0.000809</td>\n",
              "      <td>0.017009</td>\n",
              "      <td>0.014899</td>\n",
              "      <td>-0.255240</td>\n",
              "      <td>-0.179070</td>\n",
              "      <td>-0.046713</td>\n",
              "      <td>-2.0547</td>\n",
              "      <td>0.226170</td>\n",
              "      <td>0.082849</td>\n",
              "      <td>-0.211900</td>\n",
              "      <td>0.199060</td>\n",
              "      <td>0.309460</td>\n",
              "      <td>0.226880</td>\n",
              "      <td>-0.060026</td>\n",
              "      <td>-0.033334</td>\n",
              "      <td>0.038108</td>\n",
              "      <td>0.22626</td>\n",
              "      <td>0.521590</td>\n",
              "      <td>0.598710</td>\n",
              "      <td>0.45327</td>\n",
              "      <td>-0.041098</td>\n",
              "      <td>-0.402930</td>\n",
              "      <td>-0.079128</td>\n",
              "      <td>0.002534</td>\n",
              "      <td>-0.36042</td>\n",
              "      <td>0.065823</td>\n",
              "      <td>-0.010745</td>\n",
              "      <td>0.054722</td>\n",
              "      <td>0.507560</td>\n",
              "      <td>-0.646120</td>\n",
              "      <td>-0.004589</td>\n",
              "      <td>-1.01730</td>\n",
              "      <td>0.302180</td>\n",
              "      <td>-0.254030</td>\n",
              "      <td>0.095647</td>\n",
              "      <td>-0.047533</td>\n",
              "      <td>-0.324790</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.009948</td>\n",
              "      <td>-0.023906</td>\n",
              "      <td>0.047874</td>\n",
              "      <td>-0.160290</td>\n",
              "      <td>-0.072023</td>\n",
              "      <td>0.158840</td>\n",
              "      <td>-0.302680</td>\n",
              "      <td>0.040359</td>\n",
              "      <td>0.091823</td>\n",
              "      <td>0.231220</td>\n",
              "      <td>0.040294</td>\n",
              "      <td>-0.027471</td>\n",
              "      <td>0.244700</td>\n",
              "      <td>0.295670</td>\n",
              "      <td>0.069906</td>\n",
              "      <td>0.219810</td>\n",
              "      <td>-2.3806</td>\n",
              "      <td>-0.029845</td>\n",
              "      <td>0.72655</td>\n",
              "      <td>-0.16307</td>\n",
              "      <td>-0.054330</td>\n",
              "      <td>0.008776</td>\n",
              "      <td>0.003676</td>\n",
              "      <td>0.036295</td>\n",
              "      <td>0.023036</td>\n",
              "      <td>-0.057012</td>\n",
              "      <td>0.006363</td>\n",
              "      <td>-0.055003</td>\n",
              "      <td>-0.100560</td>\n",
              "      <td>0.141430</td>\n",
              "      <td>0.045239</td>\n",
              "      <td>-0.352980</td>\n",
              "      <td>0.333500</td>\n",
              "      <td>0.281040</td>\n",
              "      <td>0.203380</td>\n",
              "      <td>-0.478800</td>\n",
              "      <td>-0.039697</td>\n",
              "      <td>0.034939</td>\n",
              "      <td>-0.125990</td>\n",
              "      <td>0.218630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>him</th>\n",
              "      <td>-0.158840</td>\n",
              "      <td>0.360950</td>\n",
              "      <td>-0.113370</td>\n",
              "      <td>-0.421430</td>\n",
              "      <td>-0.284790</td>\n",
              "      <td>-0.089365</td>\n",
              "      <td>0.125030</td>\n",
              "      <td>-0.359220</td>\n",
              "      <td>0.027257</td>\n",
              "      <td>-1.6332</td>\n",
              "      <td>0.347510</td>\n",
              "      <td>-0.233640</td>\n",
              "      <td>0.041219</td>\n",
              "      <td>-0.032185</td>\n",
              "      <td>-0.199800</td>\n",
              "      <td>0.594600</td>\n",
              "      <td>-0.417830</td>\n",
              "      <td>-0.102570</td>\n",
              "      <td>0.466200</td>\n",
              "      <td>-0.13472</td>\n",
              "      <td>0.199970</td>\n",
              "      <td>0.172940</td>\n",
              "      <td>0.18640</td>\n",
              "      <td>-0.211500</td>\n",
              "      <td>-0.321580</td>\n",
              "      <td>-0.478130</td>\n",
              "      <td>-0.309680</td>\n",
              "      <td>0.46145</td>\n",
              "      <td>0.379300</td>\n",
              "      <td>0.017530</td>\n",
              "      <td>-0.120280</td>\n",
              "      <td>0.081734</td>\n",
              "      <td>0.413820</td>\n",
              "      <td>-0.399090</td>\n",
              "      <td>-1.47710</td>\n",
              "      <td>-0.327840</td>\n",
              "      <td>-0.033119</td>\n",
              "      <td>0.134780</td>\n",
              "      <td>-0.318590</td>\n",
              "      <td>-0.520440</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010122</td>\n",
              "      <td>-0.382140</td>\n",
              "      <td>0.270240</td>\n",
              "      <td>0.049569</td>\n",
              "      <td>-0.163390</td>\n",
              "      <td>-0.250100</td>\n",
              "      <td>0.036627</td>\n",
              "      <td>0.020517</td>\n",
              "      <td>-0.445570</td>\n",
              "      <td>-0.231970</td>\n",
              "      <td>-0.103870</td>\n",
              "      <td>-0.252090</td>\n",
              "      <td>0.480960</td>\n",
              "      <td>0.582080</td>\n",
              "      <td>0.610740</td>\n",
              "      <td>-0.042804</td>\n",
              "      <td>-2.2421</td>\n",
              "      <td>-0.410390</td>\n",
              "      <td>0.56638</td>\n",
              "      <td>0.24402</td>\n",
              "      <td>-0.317310</td>\n",
              "      <td>0.372080</td>\n",
              "      <td>0.400070</td>\n",
              "      <td>0.417550</td>\n",
              "      <td>-0.045318</td>\n",
              "      <td>0.577610</td>\n",
              "      <td>-0.095198</td>\n",
              "      <td>0.365310</td>\n",
              "      <td>0.041513</td>\n",
              "      <td>-0.112300</td>\n",
              "      <td>0.030342</td>\n",
              "      <td>-0.196140</td>\n",
              "      <td>-0.160160</td>\n",
              "      <td>-0.285130</td>\n",
              "      <td>0.277510</td>\n",
              "      <td>-0.143610</td>\n",
              "      <td>0.214780</td>\n",
              "      <td>-0.223810</td>\n",
              "      <td>-0.293430</td>\n",
              "      <td>-0.033526</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>united</th>\n",
              "      <td>0.056277</td>\n",
              "      <td>-0.165980</td>\n",
              "      <td>0.332410</td>\n",
              "      <td>-0.064785</td>\n",
              "      <td>0.268810</td>\n",
              "      <td>0.108790</td>\n",
              "      <td>-0.807370</td>\n",
              "      <td>0.384470</td>\n",
              "      <td>0.128930</td>\n",
              "      <td>-1.5534</td>\n",
              "      <td>0.716530</td>\n",
              "      <td>-0.186140</td>\n",
              "      <td>0.008904</td>\n",
              "      <td>-0.153880</td>\n",
              "      <td>0.295210</td>\n",
              "      <td>0.255580</td>\n",
              "      <td>-0.039114</td>\n",
              "      <td>-0.271940</td>\n",
              "      <td>-0.490910</td>\n",
              "      <td>-0.53010</td>\n",
              "      <td>0.309050</td>\n",
              "      <td>0.057643</td>\n",
              "      <td>0.63180</td>\n",
              "      <td>0.690080</td>\n",
              "      <td>-0.475290</td>\n",
              "      <td>0.231340</td>\n",
              "      <td>-0.072885</td>\n",
              "      <td>0.21801</td>\n",
              "      <td>-0.574260</td>\n",
              "      <td>0.158170</td>\n",
              "      <td>-0.078279</td>\n",
              "      <td>-0.072803</td>\n",
              "      <td>0.370130</td>\n",
              "      <td>0.219360</td>\n",
              "      <td>-0.74636</td>\n",
              "      <td>-0.166430</td>\n",
              "      <td>-0.117990</td>\n",
              "      <td>-0.091981</td>\n",
              "      <td>-0.705050</td>\n",
              "      <td>-0.076960</td>\n",
              "      <td>...</td>\n",
              "      <td>0.091354</td>\n",
              "      <td>0.539980</td>\n",
              "      <td>0.059754</td>\n",
              "      <td>0.370310</td>\n",
              "      <td>0.049488</td>\n",
              "      <td>-0.061625</td>\n",
              "      <td>-0.326090</td>\n",
              "      <td>0.036495</td>\n",
              "      <td>0.348210</td>\n",
              "      <td>0.240460</td>\n",
              "      <td>0.137350</td>\n",
              "      <td>-0.254010</td>\n",
              "      <td>-0.408870</td>\n",
              "      <td>0.196740</td>\n",
              "      <td>0.276770</td>\n",
              "      <td>0.007295</td>\n",
              "      <td>-2.3404</td>\n",
              "      <td>-0.084917</td>\n",
              "      <td>1.19050</td>\n",
              "      <td>0.65378</td>\n",
              "      <td>0.097328</td>\n",
              "      <td>-0.249420</td>\n",
              "      <td>0.298030</td>\n",
              "      <td>0.312380</td>\n",
              "      <td>0.294290</td>\n",
              "      <td>0.157960</td>\n",
              "      <td>-0.197830</td>\n",
              "      <td>0.675800</td>\n",
              "      <td>0.093091</td>\n",
              "      <td>0.068369</td>\n",
              "      <td>0.017244</td>\n",
              "      <td>-0.120950</td>\n",
              "      <td>0.501220</td>\n",
              "      <td>0.161380</td>\n",
              "      <td>0.359710</td>\n",
              "      <td>0.752880</td>\n",
              "      <td>0.479690</td>\n",
              "      <td>-0.274290</td>\n",
              "      <td>-0.171260</td>\n",
              "      <td>-0.803720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>during</th>\n",
              "      <td>-0.039635</td>\n",
              "      <td>0.212610</td>\n",
              "      <td>-0.245140</td>\n",
              "      <td>-0.266630</td>\n",
              "      <td>0.189180</td>\n",
              "      <td>0.099316</td>\n",
              "      <td>-0.370910</td>\n",
              "      <td>0.038365</td>\n",
              "      <td>-0.092625</td>\n",
              "      <td>-1.6565</td>\n",
              "      <td>-0.220160</td>\n",
              "      <td>0.182540</td>\n",
              "      <td>-0.270180</td>\n",
              "      <td>-0.367450</td>\n",
              "      <td>-0.049236</td>\n",
              "      <td>-0.035273</td>\n",
              "      <td>0.390280</td>\n",
              "      <td>-0.115900</td>\n",
              "      <td>-0.596990</td>\n",
              "      <td>-0.28118</td>\n",
              "      <td>-0.111550</td>\n",
              "      <td>-0.131290</td>\n",
              "      <td>0.40255</td>\n",
              "      <td>-0.370450</td>\n",
              "      <td>0.105820</td>\n",
              "      <td>-0.766230</td>\n",
              "      <td>-0.170740</td>\n",
              "      <td>-0.40682</td>\n",
              "      <td>-0.115560</td>\n",
              "      <td>0.115680</td>\n",
              "      <td>0.214440</td>\n",
              "      <td>-0.086590</td>\n",
              "      <td>-0.054054</td>\n",
              "      <td>0.043051</td>\n",
              "      <td>-0.77924</td>\n",
              "      <td>-0.466230</td>\n",
              "      <td>-0.153960</td>\n",
              "      <td>0.418660</td>\n",
              "      <td>0.071360</td>\n",
              "      <td>0.227000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.502810</td>\n",
              "      <td>0.099352</td>\n",
              "      <td>0.044206</td>\n",
              "      <td>0.254760</td>\n",
              "      <td>0.030103</td>\n",
              "      <td>-0.392710</td>\n",
              "      <td>-0.369050</td>\n",
              "      <td>-0.379220</td>\n",
              "      <td>0.445210</td>\n",
              "      <td>0.022462</td>\n",
              "      <td>-0.234380</td>\n",
              "      <td>-0.028043</td>\n",
              "      <td>-0.171610</td>\n",
              "      <td>0.148560</td>\n",
              "      <td>0.768720</td>\n",
              "      <td>-0.181240</td>\n",
              "      <td>-2.2793</td>\n",
              "      <td>0.050329</td>\n",
              "      <td>0.20104</td>\n",
              "      <td>0.20515</td>\n",
              "      <td>-0.880080</td>\n",
              "      <td>-0.333560</td>\n",
              "      <td>0.595430</td>\n",
              "      <td>-0.290260</td>\n",
              "      <td>0.161700</td>\n",
              "      <td>0.744840</td>\n",
              "      <td>-0.348560</td>\n",
              "      <td>0.333540</td>\n",
              "      <td>-0.247880</td>\n",
              "      <td>-0.151580</td>\n",
              "      <td>-0.148390</td>\n",
              "      <td>-0.020586</td>\n",
              "      <td>-0.697330</td>\n",
              "      <td>0.248950</td>\n",
              "      <td>0.214960</td>\n",
              "      <td>0.351270</td>\n",
              "      <td>-0.146670</td>\n",
              "      <td>-0.142210</td>\n",
              "      <td>-0.464110</td>\n",
              "      <td>-0.159580</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>before</th>\n",
              "      <td>0.000730</td>\n",
              "      <td>0.109870</td>\n",
              "      <td>0.063975</td>\n",
              "      <td>0.026729</td>\n",
              "      <td>0.100660</td>\n",
              "      <td>0.045820</td>\n",
              "      <td>-0.178650</td>\n",
              "      <td>-0.146560</td>\n",
              "      <td>0.223870</td>\n",
              "      <td>-1.5851</td>\n",
              "      <td>-0.075385</td>\n",
              "      <td>0.077855</td>\n",
              "      <td>0.071722</td>\n",
              "      <td>-0.020296</td>\n",
              "      <td>-0.214540</td>\n",
              "      <td>0.296400</td>\n",
              "      <td>0.042858</td>\n",
              "      <td>-0.243370</td>\n",
              "      <td>0.280420</td>\n",
              "      <td>0.17027</td>\n",
              "      <td>-0.153200</td>\n",
              "      <td>-0.097088</td>\n",
              "      <td>0.37339</td>\n",
              "      <td>-0.045384</td>\n",
              "      <td>-0.064616</td>\n",
              "      <td>-0.169100</td>\n",
              "      <td>0.005065</td>\n",
              "      <td>-0.19953</td>\n",
              "      <td>-0.335780</td>\n",
              "      <td>0.082778</td>\n",
              "      <td>0.172250</td>\n",
              "      <td>-0.047207</td>\n",
              "      <td>0.367810</td>\n",
              "      <td>0.123790</td>\n",
              "      <td>-1.37300</td>\n",
              "      <td>-0.180220</td>\n",
              "      <td>-0.383480</td>\n",
              "      <td>0.340670</td>\n",
              "      <td>-0.236780</td>\n",
              "      <td>-0.080015</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.203530</td>\n",
              "      <td>-0.091527</td>\n",
              "      <td>0.167280</td>\n",
              "      <td>0.188500</td>\n",
              "      <td>0.216750</td>\n",
              "      <td>0.240210</td>\n",
              "      <td>-0.263730</td>\n",
              "      <td>-0.542920</td>\n",
              "      <td>0.034036</td>\n",
              "      <td>-0.083158</td>\n",
              "      <td>-0.216980</td>\n",
              "      <td>-0.050733</td>\n",
              "      <td>0.134110</td>\n",
              "      <td>0.323780</td>\n",
              "      <td>0.379540</td>\n",
              "      <td>0.028645</td>\n",
              "      <td>-2.2839</td>\n",
              "      <td>-0.161530</td>\n",
              "      <td>0.36115</td>\n",
              "      <td>-0.12518</td>\n",
              "      <td>-0.436550</td>\n",
              "      <td>-0.198100</td>\n",
              "      <td>0.282980</td>\n",
              "      <td>0.003422</td>\n",
              "      <td>-0.090104</td>\n",
              "      <td>0.556460</td>\n",
              "      <td>-0.138800</td>\n",
              "      <td>0.090241</td>\n",
              "      <td>-0.111960</td>\n",
              "      <td>0.068667</td>\n",
              "      <td>0.124490</td>\n",
              "      <td>-0.147260</td>\n",
              "      <td>0.078155</td>\n",
              "      <td>-0.052333</td>\n",
              "      <td>0.012701</td>\n",
              "      <td>0.176650</td>\n",
              "      <td>0.020622</td>\n",
              "      <td>-0.481250</td>\n",
              "      <td>-0.078685</td>\n",
              "      <td>0.200680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>may</th>\n",
              "      <td>-0.376040</td>\n",
              "      <td>0.241160</td>\n",
              "      <td>-0.260980</td>\n",
              "      <td>-0.007960</td>\n",
              "      <td>0.219800</td>\n",
              "      <td>0.098515</td>\n",
              "      <td>0.234580</td>\n",
              "      <td>0.137970</td>\n",
              "      <td>0.110260</td>\n",
              "      <td>-1.7503</td>\n",
              "      <td>0.157280</td>\n",
              "      <td>0.500990</td>\n",
              "      <td>-0.070068</td>\n",
              "      <td>0.023731</td>\n",
              "      <td>0.398480</td>\n",
              "      <td>0.344060</td>\n",
              "      <td>0.272500</td>\n",
              "      <td>-0.311730</td>\n",
              "      <td>-0.083764</td>\n",
              "      <td>-0.22098</td>\n",
              "      <td>-0.270320</td>\n",
              "      <td>-0.061412</td>\n",
              "      <td>0.23487</td>\n",
              "      <td>-0.108980</td>\n",
              "      <td>-0.500680</td>\n",
              "      <td>-0.191880</td>\n",
              "      <td>-0.081558</td>\n",
              "      <td>-0.49529</td>\n",
              "      <td>-0.112060</td>\n",
              "      <td>0.055647</td>\n",
              "      <td>0.347620</td>\n",
              "      <td>0.381170</td>\n",
              "      <td>0.251340</td>\n",
              "      <td>0.221550</td>\n",
              "      <td>-0.60474</td>\n",
              "      <td>0.149160</td>\n",
              "      <td>0.034044</td>\n",
              "      <td>0.080041</td>\n",
              "      <td>-0.025968</td>\n",
              "      <td>-0.024241</td>\n",
              "      <td>...</td>\n",
              "      <td>0.005466</td>\n",
              "      <td>-0.511980</td>\n",
              "      <td>-0.080661</td>\n",
              "      <td>0.069866</td>\n",
              "      <td>-0.167940</td>\n",
              "      <td>-0.519120</td>\n",
              "      <td>0.156840</td>\n",
              "      <td>-0.000717</td>\n",
              "      <td>0.101390</td>\n",
              "      <td>0.162740</td>\n",
              "      <td>0.189130</td>\n",
              "      <td>0.157990</td>\n",
              "      <td>0.557480</td>\n",
              "      <td>-0.051792</td>\n",
              "      <td>-0.142220</td>\n",
              "      <td>0.084011</td>\n",
              "      <td>-1.8705</td>\n",
              "      <td>-0.293800</td>\n",
              "      <td>0.42032</td>\n",
              "      <td>0.09180</td>\n",
              "      <td>-0.263130</td>\n",
              "      <td>0.125250</td>\n",
              "      <td>0.449980</td>\n",
              "      <td>0.363530</td>\n",
              "      <td>-0.196110</td>\n",
              "      <td>0.127600</td>\n",
              "      <td>-0.304500</td>\n",
              "      <td>-0.061133</td>\n",
              "      <td>-0.027884</td>\n",
              "      <td>-0.011002</td>\n",
              "      <td>-0.083795</td>\n",
              "      <td>-0.158100</td>\n",
              "      <td>0.325670</td>\n",
              "      <td>-0.429300</td>\n",
              "      <td>0.110570</td>\n",
              "      <td>0.312300</td>\n",
              "      <td>-0.255790</td>\n",
              "      <td>-0.757310</td>\n",
              "      <td>0.097391</td>\n",
              "      <td>-0.081830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>since</th>\n",
              "      <td>0.132110</td>\n",
              "      <td>0.009039</td>\n",
              "      <td>0.464310</td>\n",
              "      <td>0.160600</td>\n",
              "      <td>0.289240</td>\n",
              "      <td>-0.014632</td>\n",
              "      <td>-0.309260</td>\n",
              "      <td>-0.102920</td>\n",
              "      <td>0.319730</td>\n",
              "      <td>-2.0459</td>\n",
              "      <td>0.107910</td>\n",
              "      <td>0.271230</td>\n",
              "      <td>0.133140</td>\n",
              "      <td>-0.058571</td>\n",
              "      <td>-0.155090</td>\n",
              "      <td>0.048896</td>\n",
              "      <td>0.088392</td>\n",
              "      <td>0.303050</td>\n",
              "      <td>-0.296530</td>\n",
              "      <td>-0.25901</td>\n",
              "      <td>0.056790</td>\n",
              "      <td>0.126440</td>\n",
              "      <td>0.20284</td>\n",
              "      <td>-0.256380</td>\n",
              "      <td>-0.037751</td>\n",
              "      <td>-0.214050</td>\n",
              "      <td>-0.373020</td>\n",
              "      <td>-0.62119</td>\n",
              "      <td>-0.211920</td>\n",
              "      <td>0.307760</td>\n",
              "      <td>-0.045039</td>\n",
              "      <td>0.037847</td>\n",
              "      <td>-0.247120</td>\n",
              "      <td>0.274960</td>\n",
              "      <td>-0.63936</td>\n",
              "      <td>-0.494560</td>\n",
              "      <td>-0.028717</td>\n",
              "      <td>-0.003221</td>\n",
              "      <td>-0.013735</td>\n",
              "      <td>0.265160</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.501370</td>\n",
              "      <td>0.339590</td>\n",
              "      <td>-0.161930</td>\n",
              "      <td>-0.174690</td>\n",
              "      <td>-0.330710</td>\n",
              "      <td>0.261330</td>\n",
              "      <td>-0.127400</td>\n",
              "      <td>-0.204760</td>\n",
              "      <td>0.395360</td>\n",
              "      <td>0.002654</td>\n",
              "      <td>-0.294450</td>\n",
              "      <td>0.572960</td>\n",
              "      <td>0.158950</td>\n",
              "      <td>-0.381390</td>\n",
              "      <td>0.379730</td>\n",
              "      <td>-0.152720</td>\n",
              "      <td>-2.1216</td>\n",
              "      <td>-0.410480</td>\n",
              "      <td>0.18469</td>\n",
              "      <td>0.61325</td>\n",
              "      <td>-0.363560</td>\n",
              "      <td>-0.434270</td>\n",
              "      <td>0.051726</td>\n",
              "      <td>-0.023941</td>\n",
              "      <td>-0.085872</td>\n",
              "      <td>0.115160</td>\n",
              "      <td>-0.652090</td>\n",
              "      <td>-0.029655</td>\n",
              "      <td>-0.154160</td>\n",
              "      <td>0.085021</td>\n",
              "      <td>0.104320</td>\n",
              "      <td>-0.210160</td>\n",
              "      <td>0.261070</td>\n",
              "      <td>0.085519</td>\n",
              "      <td>0.171470</td>\n",
              "      <td>0.854160</td>\n",
              "      <td>0.069214</td>\n",
              "      <td>-0.649110</td>\n",
              "      <td>0.410390</td>\n",
              "      <td>0.080164</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>many</th>\n",
              "      <td>-0.155070</td>\n",
              "      <td>0.284220</td>\n",
              "      <td>0.311910</td>\n",
              "      <td>-0.195960</td>\n",
              "      <td>0.125730</td>\n",
              "      <td>-0.152090</td>\n",
              "      <td>0.145480</td>\n",
              "      <td>0.337110</td>\n",
              "      <td>-0.093931</td>\n",
              "      <td>-1.4419</td>\n",
              "      <td>0.101740</td>\n",
              "      <td>0.192890</td>\n",
              "      <td>-0.172980</td>\n",
              "      <td>0.237980</td>\n",
              "      <td>0.539470</td>\n",
              "      <td>-0.378290</td>\n",
              "      <td>-0.337830</td>\n",
              "      <td>-0.060360</td>\n",
              "      <td>0.162360</td>\n",
              "      <td>0.13202</td>\n",
              "      <td>0.063932</td>\n",
              "      <td>0.100850</td>\n",
              "      <td>0.54784</td>\n",
              "      <td>0.292160</td>\n",
              "      <td>-0.425790</td>\n",
              "      <td>0.035594</td>\n",
              "      <td>-0.179820</td>\n",
              "      <td>0.28462</td>\n",
              "      <td>-0.054354</td>\n",
              "      <td>0.345400</td>\n",
              "      <td>0.561260</td>\n",
              "      <td>0.528300</td>\n",
              "      <td>-0.516910</td>\n",
              "      <td>-0.146380</td>\n",
              "      <td>-0.66691</td>\n",
              "      <td>-0.374670</td>\n",
              "      <td>0.079261</td>\n",
              "      <td>0.177150</td>\n",
              "      <td>0.273670</td>\n",
              "      <td>-0.376340</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.106620</td>\n",
              "      <td>-0.127140</td>\n",
              "      <td>0.436380</td>\n",
              "      <td>-0.094561</td>\n",
              "      <td>0.255130</td>\n",
              "      <td>-0.314150</td>\n",
              "      <td>-0.638040</td>\n",
              "      <td>-0.177540</td>\n",
              "      <td>0.196950</td>\n",
              "      <td>0.379490</td>\n",
              "      <td>-0.058079</td>\n",
              "      <td>0.570330</td>\n",
              "      <td>0.045701</td>\n",
              "      <td>0.017218</td>\n",
              "      <td>0.075382</td>\n",
              "      <td>0.005122</td>\n",
              "      <td>-2.7359</td>\n",
              "      <td>-0.099892</td>\n",
              "      <td>0.33327</td>\n",
              "      <td>0.22997</td>\n",
              "      <td>-0.201590</td>\n",
              "      <td>0.040204</td>\n",
              "      <td>0.119150</td>\n",
              "      <td>0.054997</td>\n",
              "      <td>-0.137480</td>\n",
              "      <td>0.216060</td>\n",
              "      <td>-0.058659</td>\n",
              "      <td>0.307390</td>\n",
              "      <td>0.219530</td>\n",
              "      <td>-0.037842</td>\n",
              "      <td>-0.124250</td>\n",
              "      <td>-0.531690</td>\n",
              "      <td>-0.037670</td>\n",
              "      <td>0.150680</td>\n",
              "      <td>-0.139130</td>\n",
              "      <td>-0.088808</td>\n",
              "      <td>-0.227130</td>\n",
              "      <td>-0.545620</td>\n",
              "      <td>-0.147350</td>\n",
              "      <td>-0.314930</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 300 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             1         2         3    ...       298       299       300\n",
              "so     -0.245610  0.068010  0.182540  ... -0.669140 -0.034753  0.097869\n",
              "them   -0.205880  0.192570 -0.118270  ... -0.328680 -0.169600 -0.129650\n",
              "what   -0.200170  0.143020  0.052055  ...  0.034939 -0.125990  0.218630\n",
              "him    -0.158840  0.360950 -0.113370  ... -0.223810 -0.293430 -0.033526\n",
              "united  0.056277 -0.165980  0.332410  ... -0.274290 -0.171260 -0.803720\n",
              "during -0.039635  0.212610 -0.245140  ... -0.142210 -0.464110 -0.159580\n",
              "before  0.000730  0.109870  0.063975  ... -0.481250 -0.078685  0.200680\n",
              "may    -0.376040  0.241160 -0.260980  ... -0.757310  0.097391 -0.081830\n",
              "since   0.132110  0.009039  0.464310  ... -0.649110  0.410390  0.080164\n",
              "many   -0.155070  0.284220  0.311910  ... -0.545620 -0.147350 -0.314930\n",
              "\n",
              "[10 rows x 300 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vz-XoubOV1li",
        "outputId": "bd7b99fd-9063-40f7-d37e-444ccc70b55a"
      },
      "source": [
        "print(word2index['sos']), print(word2index['eos'])\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0\n",
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC78oNh6r5bp"
      },
      "source": [
        "#word2index = { k : v for k , v in sorted(word2index.items(), key=operator.itemgetter(1))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZ6wu_qLsTve"
      },
      "source": [
        "class InputLang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = { k : v for k , v in sorted(word2index.items(), key=operator.itemgetter(1))}\n",
        "        self.word2count = { word : 1 for word in words_list }\n",
        "        self.index2word = { i : word for word, i in word2index.items() }\n",
        "        self.n_words = 400001\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A14Rcm5csW6Q"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "class OutputLang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {0: \"sos\", 1: \"eos\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "    def addSentence(self, sentence):\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "    def addWord(self, word):\n",
        "        if word not in self.word2index:\n",
        "            self.word2index[word] = self.n_words\n",
        "            self.word2count[word] = 1\n",
        "            self.index2word[self.n_words] = word\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[word] += 1"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxcKlkaxsT6r"
      },
      "source": [
        "# Turn a Unicode string to plain ASCII, thanks to\n",
        "# http://stackoverflow.com/a/518232/2809427\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "    )\n",
        "# Lowercase, trim, and remove non-letter characters\n",
        "def normalizeString(s):\n",
        "    s = unicodeToAscii(s.lower().strip())\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dESM3mrshHp"
      },
      "source": [
        "def readLangs(lang1, lang2, reverse=False):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # Read the file and split into lines\n",
        "    lines = open('data/%s-%s.txt' % (lang1, lang2), encoding='utf-8').\\\n",
        "        read().strip().split('\\n')\n",
        "\n",
        "    # Split every line into pairs and normalize\n",
        "    pairs = [[normalizeString(s) for s in l.split('\\t')] for l in lines]\n",
        "    #print(pairs[0])\n",
        "\n",
        "    # Reverse pairs, make Lang instances\n",
        "    if reverse:\n",
        "        pairs = [list(reversed(p)) for p in pairs]\n",
        "        input_lang = InputLang(lang2)\n",
        "        output_lang = OutputLang(lang1)\n",
        "    else:\n",
        "        input_lang = InputLang(lang1)\n",
        "        print(input_lang)\n",
        "        output_lang = OutputLang(lang2)\n",
        "\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMo2grUWswqt"
      },
      "source": [
        "MAX_LENGTH = 10\n",
        "\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "\n",
        "def filterPair(p,reverse):\n",
        "  if(reverse):\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[1].startswith(eng_prefixes)\n",
        "  else:\n",
        "     return len(p[0].split(' ')) < MAX_LENGTH and \\\n",
        "        len(p[1].split(' ')) < MAX_LENGTH and \\\n",
        "        p[0].startswith(eng_prefixes)\n",
        "\n",
        "\n",
        "\n",
        "def filterPairs(pairs,reverse=False):\n",
        "    return [pair for pair in pairs if filterPair(pair,reverse)]"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iVRUbt1WtI0y",
        "outputId": "691d07b4-079d-4c21-fcc9-3a651acd5964"
      },
      "source": [
        "def prepareData(lang1, lang2, reverse=False):\n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2, reverse)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs,reverse)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "    print(\"Counting words...\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "    print(\"Counted words:\")\n",
        "    print(input_lang.name, input_lang.n_words)\n",
        "    print(output_lang.name, output_lang.n_words)\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, pairs = prepareData('eng', 'fra')\n",
        "print(random.choice(pairs))\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading lines...\n",
            "<__main__.InputLang object at 0x7fc069301910>\n",
            "Read 135842 sentence pairs\n",
            "Trimmed to 10599 sentence pairs\n",
            "Counting words...\n",
            "Counted words:\n",
            "eng 400005\n",
            "fra 4345\n",
            "['you re too skinny .', 'vous etes trop maigrichonnes .']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EOhQo9iOvE5z"
      },
      "source": [
        "matrix_len = input_lang.n_words\n",
        "weights_matrix = np.zeros((matrix_len, 300))\n",
        "words_found = 0\n",
        "for i, word in enumerate(input_lang.word2index):\n",
        "    try: \n",
        "        weights_matrix[i] = glove[word]\n",
        "        words_found += 1\n",
        "    except KeyError:\n",
        "        weights_matrix[i] = np.random.normal(scale=0.6, size=(300, ))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sklKeEAZEw9A"
      },
      "source": [
        "The Seq2Seq Model\n",
        "=================\n",
        "\n",
        "A Recurrent Neural Network, or RNN, is a network that operates on a\n",
        "sequence and uses its own output as input for subsequent steps.\n",
        "\n",
        "A [Sequence to Sequence network](https://arxiv.org/abs/1409.3215), or\n",
        "seq2seq network, or [Encoder Decoder\n",
        "network](https://arxiv.org/pdf/1406.1078v3.pdf), is a model\n",
        "consisting of two RNNs called the encoder and decoder. The encoder reads\n",
        "an input sequence and outputs a single vector, and the decoder reads\n",
        "that vector to produce an output sequence.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/seq2seq.png)\n",
        "\n",
        "Unlike sequence prediction with a single RNN, where every input\n",
        "corresponds to an output, the seq2seq model frees us from sequence\n",
        "length and order, which makes it ideal for translation between two\n",
        "languages.\n",
        "\n",
        "Consider the sentence \"Je ne suis pas le chat noir\" → \"I am not the\n",
        "black cat\". Most of the words in the input sentence have a direct\n",
        "translation in the output sentence, but are in slightly different\n",
        "orders, e.g. \"chat noir\" and \"black cat\". Because of the \"ne/pas\"\n",
        "construction there is also one more word in the input sentence. It would\n",
        "be difficult to produce a correct translation directly from the sequence\n",
        "of input words.\n",
        "\n",
        "With a seq2seq model the encoder creates a single vector which, in the\n",
        "ideal case, encodes the \"meaning\" of the input sequence into a single\n",
        "vector — a single point in some N dimensional space of sentences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJZ7qgeoGH25"
      },
      "source": [
        "The Encoder\n",
        "-----------\n",
        "\n",
        "The encoder of a seq2seq network is a RNN that outputs some value for\n",
        "every word from the input sentence. For every input word the encoder\n",
        "outputs a vector and a hidden state, and uses the hidden state for the\n",
        "next input word.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/encoder-network.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPEai_3-CiSe",
        "outputId": "60b7e376-de99-4b50-9ec1-77459ecfb35c"
      },
      "source": [
        "type(weights_matrix)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYWs0A8aClE_",
        "outputId": "121749ac-6a81-4a96-f435-bb7031a91a06"
      },
      "source": [
        "type(torch.Tensor(weights_matrix))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Tensor"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUFStkIuEqo7"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        #self.embedding.weight.data.copy_(torch.from_numpy(weights_matrix))\n",
        "        self.embedding = nn.Embedding.from_pretrained(torch.Tensor(weights_matrix), freeze=False)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WzcqRwLGR_c"
      },
      "source": [
        "#Simple Decoder\n",
        "\n",
        "In the simplest seq2seq decoder we use only last output of the encoder.\n",
        "This last output is sometimes called the *context vector* as it encodes\n",
        "context from the entire sequence. This context vector is used as the\n",
        "initial hidden state of the decoder.\n",
        "\n",
        "At every step of decoding, the decoder is given an input token and\n",
        "hidden state. The initial input token is the start-of-string ``<SOS>``\n",
        "token, and the first hidden state is the context vector (the encoder's\n",
        "last hidden state).\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/decoder-network.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aTpja5ExGQR_"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(output_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iGMxRg_-Ga5T"
      },
      "source": [
        "# Attention Decoder\n",
        "\n",
        "If only the context vector is passed between the encoder and decoder,\n",
        "that single vector carries the burden of encoding the entire sentence.\n",
        "\n",
        "Attention allows the decoder network to \"focus\" on a different part of\n",
        "the encoder's outputs for every step of the decoder's own outputs. First\n",
        "we calculate a set of *attention weights*. These will be multiplied by\n",
        "the encoder output vectors to create a weighted combination. The result\n",
        "(called ``attn_applied`` in the code) should contain information about\n",
        "that specific part of the input sequence, and thus help the decoder\n",
        "choose the right output words.\n",
        "\n",
        "![image](https://i.imgur.com/1152PYf.png)\n",
        "\n",
        "Calculating the attention weights is done with another feed-forward\n",
        "layer ``attn``, using the decoder's input and hidden state as inputs.\n",
        "Because there are sentences of all sizes in the training data, to\n",
        "actually create and train this layer we have to choose a maximum\n",
        "sentence length (input length, for encoder outputs) that it can apply\n",
        "to. Sentences of the maximum length will use all the attention weights,\n",
        "while shorter sentences will only use the first few.\n",
        "\n",
        "![image](https://pytorch.org/tutorials/_images/attention-decoder-network.png)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTgpqalYGaOR"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Xa7siQKGqQE"
      },
      "source": [
        "<div class=\"alert alert-info\"><h4>Note</h4><p>There are other forms of attention that work around the length\n",
        "  limitation by using a relative position approach. Read about \"local\n",
        "  attention\" in `Effective Approaches to Attention-based Neural Machine\n",
        "  Translation <https://arxiv.org/abs/1508.04025>`__.</p></div>\n",
        "\n",
        "Training\n",
        "========\n",
        "\n",
        "Preparing Training Data\n",
        "-----------------------\n",
        "\n",
        "To train, for each pair we will need an input tensor (indexes of the\n",
        "words in the input sentence) and target tensor (indexes of the words in\n",
        "the target sentence). While creating these vectors we will append the\n",
        "EOS token to both sequences.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mj3FhJBGoS_"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    return [lang.word2index[word] for word in sentence.split(' ')]\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rtejHTjG5Ia"
      },
      "source": [
        "Training the Model\n",
        "------------------\n",
        "\n",
        "To train we run the input sentence through the encoder, and keep track\n",
        "of every output and the latest hidden state. Then the decoder is given\n",
        "the ``<SOS>`` token as its first input, and the last hidden state of the\n",
        "encoder as its first hidden state.\n",
        "\n",
        "\"Teacher forcing\" is the concept of using the real target outputs as\n",
        "each next input, instead of using the decoder's guess as the next input.\n",
        "Using teacher forcing causes it to converge faster but [when the trained network is exploited, it may exhibit instability](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.378.4095&rep=rep1&type=pdf).\n",
        "\n",
        "You can observe outputs of teacher-forced networks that read with\n",
        "coherent grammar but wander far from the correct translation -\n",
        "intuitively it has learned to represent the output grammar and can \"pick\n",
        "up\" the meaning once the teacher tells it the first few words, but it\n",
        "has not properly learned how to create the sentence from the translation\n",
        "in the first place.\n",
        "\n",
        "Because of the freedom PyTorch's autograd gives us, we can randomly\n",
        "choose to use teacher forcing or not with a simple if statement. Turn\n",
        "``teacher_forcing_ratio`` up to use more of it.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xWVqaIu-Gt3b"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5OQi5ocOHDuv"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0A3z6eQtG3rv"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "po1iCyBTHHIG"
      },
      "source": [
        "The whole training process looks like this:\n",
        "\n",
        "-  Start a timer\n",
        "-  Initialize optimizers and criterion\n",
        "-  Create set of training pairs\n",
        "-  Start empty losses array for plotting\n",
        "\n",
        "Then we call ``train`` many times and occasionally print the progress (%\n",
        "of examples, time so far, estimated time) and average loss.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Gf1aSD1HFx_"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SxLPtvy-HJv8"
      },
      "source": [
        "Plotting results\n",
        "----------------\n",
        "\n",
        "Plotting is done with matplotlib, using the array of loss values\n",
        "``plot_losses`` saved while training.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kErPEK3PHIjD"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8BWg0QVLHMnS"
      },
      "source": [
        "Evaluation\n",
        "==========\n",
        "\n",
        "Evaluation is mostly the same as training, but there are no targets so\n",
        "we simply feed the decoder's predictions back to itself for each step.\n",
        "Every time it predicts a word we add it to the output string, and if it\n",
        "predicts the EOS token we stop there. We also store the decoder's\n",
        "attention outputs for display later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbjgrd4yHLMS"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tEDyINR3HP8X"
      },
      "source": [
        "We can evaluate random sentences from the training set and print out the\n",
        "input, target, and output to make some subjective quality judgements:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "41NOzzmRHOfs"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I3YLTtFHSkq"
      },
      "source": [
        "Training and Evaluating\n",
        "=======================\n",
        "\n",
        "With all these helper functions in place (it looks like extra work, but\n",
        "it makes it easier to run multiple experiments) we can actually\n",
        "initialize a network and start training.\n",
        "\n",
        "Remember that the input sentences were heavily filtered. For this small\n",
        "dataset we can use relatively small networks of 256 hidden nodes and a\n",
        "single GRU layer. After about 40 minutes on a MacBook CPU we'll get some\n",
        "reasonable results.\n",
        "\n",
        ".. Note::\n",
        "   If you run this notebook you can train, interrupt the kernel,\n",
        "   evaluate, and continue training later. Comment out the lines where the\n",
        "   encoder and decoder are initialized and run ``trainIters`` again.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJoLJ8ARwsda",
        "outputId": "5b1d639f-873e-4027-d871-3d768111b612"
      },
      "source": [
        "device"
      ],
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 76
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-45b8_lHRiK",
        "outputId": "2c63c957-c96f-4ce6-fa26-3d3a9174c07f"
      },
      "source": [
        "hidden_size = 300\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.3).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 90000, print_every=5000)"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2m 41s (- 45m 45s) (5000 5%) 3.6272\n",
            "5m 18s (- 42m 25s) (10000 11%) 3.0602\n",
            "7m 56s (- 39m 43s) (15000 16%) 2.8022\n",
            "10m 33s (- 36m 57s) (20000 22%) 2.5785\n",
            "13m 11s (- 34m 18s) (25000 27%) 2.4144\n",
            "15m 49s (- 31m 39s) (30000 33%) 2.2028\n",
            "18m 28s (- 29m 1s) (35000 38%) 2.0771\n",
            "21m 6s (- 26m 22s) (40000 44%) 1.9515\n",
            "23m 44s (- 23m 44s) (45000 50%) 1.7920\n",
            "26m 22s (- 21m 6s) (50000 55%) 1.6828\n",
            "29m 0s (- 18m 27s) (55000 61%) 1.5564\n",
            "31m 38s (- 15m 49s) (60000 66%) 1.4653\n",
            "34m 16s (- 13m 11s) (65000 72%) 1.3440\n",
            "36m 54s (- 10m 32s) (70000 77%) 1.2892\n",
            "39m 32s (- 7m 54s) (75000 83%) 1.1816\n",
            "42m 11s (- 5m 16s) (80000 88%) 1.1049\n",
            "44m 49s (- 2m 38s) (85000 94%) 1.0365\n",
            "47m 28s (- 0m 0s) (90000 100%) 1.0132\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e_rlcrR1Iqld",
        "outputId": "c01785bc-9b36-4033-e8cf-96903688f6ad"
      },
      "source": [
        "evaluateRandomly(encoder1, attn_decoder1)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "> they re right behind you .\n",
            "= elles se trouvent juste derriere toi .\n",
            "< elles sont juste derriere toi . <EOS>\n",
            "\n",
            "> i m lucky today .\n",
            "= je suis chanceux aujourd hui .\n",
            "< je suis chanceux aujourd hui . <EOS>\n",
            "\n",
            "> i m dancing .\n",
            "= je suis en train de danser .\n",
            "< je suis en train de danser . <EOS>\n",
            "\n",
            "> i m just glad everything worked out .\n",
            "= tout est bien qui finit bien .\n",
            "< je suis sur bien tout . <EOS>\n",
            "\n",
            "> they are spraying the fruit trees .\n",
            "= ils epandent les arbres fruitiers .\n",
            "< ils epandent les arbres fruitiers . <EOS>\n",
            "\n",
            "> you re worried aren t you ?\n",
            "= vous etes inquietes n est ce pas ?\n",
            "< tu es inquiete n est ce pas ? <EOS>\n",
            "\n",
            "> you re wasting ammo .\n",
            "= vous gaspillez des munitions .\n",
            "< tu gaspillez des munitions . <EOS>\n",
            "\n",
            "> he s a very talented man .\n",
            "= c est un homme tres talentueux .\n",
            "< c est un homme tres talentueux . <EOS>\n",
            "\n",
            "> i m being paid to do this .\n",
            "= je suis paye pour faire ca .\n",
            "< je suis surpris a faire faire faire cela . <EOS>\n",
            "\n",
            "> i m observant .\n",
            "= je suis respectueuse .\n",
            "< je suis respectueuse . <EOS>\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4J1zvrwInJk"
      },
      "source": [
        "Visualizing Attention\n",
        "---------------------\n",
        "\n",
        "A useful property of the attention mechanism is its highly interpretable\n",
        "outputs. Because it is used to weight specific encoder outputs of the\n",
        "input sequence, we can imagine looking where the network is focused most\n",
        "at each time step.\n",
        "\n",
        "You could simply run ``plt.matshow(attentions)`` to see attention output\n",
        "displayed as a matrix, with the columns being input steps and rows being\n",
        "output steps:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "id": "1t-jIrj1HVL9",
        "outputId": "d9bb2b03-c3a8-44a5-d13f-f962ae341534"
      },
      "source": [
        "%matplotlib inline\n",
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"you re not going fast enough .\")\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7fc974909510>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 360x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5zYmdIDIvbR"
      },
      "source": [
        "For a better viewing experience we will do the extra work of adding axes\n",
        "and labels:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6iRp2UgrIxPQ",
        "outputId": "f82c3286-42dd-490c-e395-984a06571058"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"you re old .\")\n",
        "\n",
        "evaluateAndShowAttention(\"you re very timid .\")\n",
        "\n",
        "evaluateAndShowAttention(\"you are so stupid .\")\n",
        "\n",
        "evaluateAndShowAttention(\"i m fussy .\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "input = you re old .\n",
            "output = tu es vieille . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "input = you re very timid .\n",
            "output = vous etes tres craintive . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "input = you are so stupid .\n",
            "output = vous etes tellement idiot ! <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "input = i m fussy .\n",
            "output = je suis au . . <EOS>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RM_7deQ7PB2u"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}