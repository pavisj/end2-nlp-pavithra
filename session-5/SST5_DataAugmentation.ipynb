{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "center-collect",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytreebank\n",
      "  Downloading pytreebank-0.2.7.tar.gz (34 kB)\n",
      "Building wheels for collected packages: pytreebank\n",
      "  Building wheel for pytreebank (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytreebank: filename=pytreebank-0.2.7-py3-none-any.whl size=37071 sha256=3369f26ca14bcbb2c614004b648f42db767b334df3b0fe6aa9fc415001acdc22\n",
      "  Stored in directory: /home/ubuntu/.cache/pip/wheels/a1/37/4d/a489a8c9045c55f783a3d49ee2354484e4c81c3414c0e4849a\n",
      "Successfully built pytreebank\n",
      "Installing collected packages: pytreebank\n",
      "Successfully installed pytreebank-0.2.7\n"
     ]
    }
   ],
   "source": [
    "!pip install pytreebank\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "together-forty",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Rock is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal . has sentiment label positive\n",
      "The Rock has sentiment label neutral\n",
      "The has sentiment label neutral\n",
      "Rock has sentiment label neutral\n",
      "is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal . has sentiment label very positive\n",
      "is destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal has sentiment label positive\n",
      "is has sentiment label neutral\n",
      "destined to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal has sentiment label very positive\n",
      "destined has sentiment label neutral\n",
      "to be the 21st Century 's new `` Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal has sentiment label neutral\n",
      "to be the 21st Century 's new `` Conan '' and has sentiment label neutral\n",
      "to be the 21st Century 's new `` Conan '' has sentiment label neutral\n",
      "to be the 21st Century 's new `` Conan has sentiment label neutral\n",
      "to has sentiment label neutral\n",
      "be the 21st Century 's new `` Conan has sentiment label neutral\n",
      "be has sentiment label neutral\n",
      "the 21st Century 's new `` Conan has sentiment label neutral\n",
      "the has sentiment label neutral\n",
      "21st Century 's new `` Conan has sentiment label neutral\n",
      "21st has sentiment label neutral\n",
      "Century 's new `` Conan has sentiment label neutral\n",
      "Century 's has sentiment label neutral\n",
      "Century has sentiment label neutral\n",
      "'s has sentiment label neutral\n",
      "new `` Conan has sentiment label neutral\n",
      "new has sentiment label positive\n",
      "`` Conan has sentiment label neutral\n",
      "`` has sentiment label neutral\n",
      "Conan has sentiment label neutral\n",
      "'' has sentiment label neutral\n",
      "and has sentiment label neutral\n",
      "that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal has sentiment label positive\n",
      "that has sentiment label neutral\n",
      "he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal has sentiment label positive\n",
      "he has sentiment label neutral\n",
      "'s going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal has sentiment label positive\n",
      "'s has sentiment label neutral\n",
      "going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal has sentiment label positive\n",
      "going has sentiment label neutral\n",
      "to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal has sentiment label positive\n",
      "to has sentiment label neutral\n",
      "make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal has sentiment label very positive\n",
      "make a splash even greater has sentiment label positive\n",
      "make has sentiment label neutral\n",
      "a splash even greater has sentiment label positive\n",
      "a splash has sentiment label positive\n",
      "a has sentiment label neutral\n",
      "splash has sentiment label positive\n",
      "even greater has sentiment label neutral\n",
      "even has sentiment label neutral\n",
      "greater has sentiment label positive\n",
      "than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal has sentiment label neutral\n",
      "than has sentiment label neutral\n",
      "Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal has sentiment label neutral\n",
      "Arnold Schwarzenegger , Jean-Claud Van Damme or has sentiment label neutral\n",
      "Arnold Schwarzenegger , Jean-Claud Van Damme has sentiment label neutral\n",
      "Arnold Schwarzenegger , has sentiment label neutral\n",
      "Arnold Schwarzenegger has sentiment label negative\n",
      "Arnold has sentiment label neutral\n",
      "Schwarzenegger has sentiment label neutral\n",
      ", has sentiment label neutral\n",
      "Jean-Claud Van Damme has sentiment label neutral\n",
      "Jean-Claud has sentiment label neutral\n",
      "Van Damme has sentiment label neutral\n",
      "Van has sentiment label neutral\n",
      "Damme has sentiment label neutral\n",
      "or has sentiment label neutral\n",
      "Steven Segal has sentiment label neutral\n",
      "Steven has sentiment label neutral\n",
      "Segal has sentiment label neutral\n",
      ". has sentiment label neutral\n"
     ]
    }
   ],
   "source": [
    "import pytreebank\n",
    "dataset = pytreebank.load_sst()\n",
    "example = dataset[\"train\"][0]\n",
    "\n",
    "# extract spans from the tree.\n",
    "for label, sentence in example.to_labeled_lines():\n",
    "    print(\"%s has sentiment label %s\" % (sentence,[\"very negative\", \"negative\", \"neutral\", \"positive\", \"very positive\"][label]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "matched-tissue",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train', 'test', 'dev']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[key for key in dataset.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "private-matrix",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pytreebank.labeled_trees.LabeledTree at 0x7fe221045fd0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "knowing-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(columns=[\"Sentence\",\"Label\",\"Dataset\"])\n",
    "df = df.astype(dtype={'Sentence':str,'Label':int,'Dataset':str})\n",
    "\n",
    "for key_value in dataset.keys():\n",
    "    for i,lbl_tree in enumerate(dataset[key_value]):\n",
    "        #print(lbl_tree.to_labeled_lines()[0])\n",
    "        label,sentence = lbl_tree.to_labeled_lines()[0]\n",
    "        df = df.append({\"Sentence\":sentence,\"Label\":label,\"Dataset\":key_value}, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "opposed-basement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the Dataset:  11855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1    3140\n",
       "3    3111\n",
       "2    2242\n",
       "4    1852\n",
       "0    1510\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Length of the Dataset: \",len(df))\n",
    "df.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "substantial-dryer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences in train:  8544\n",
      "Number of sentences in test:  2210\n",
      "Number of sentences in dev:  1101\n"
     ]
    }
   ],
   "source": [
    "df.to_csv(\"dataset/sst_dataset.csv\")\n",
    "for key in dataset.keys():\n",
    "    df_ = df[df[\"Dataset\"]==key]\n",
    "    df_.to_csv(\"dataset/\" + key+\".csv\")\n",
    "    print(\"Number of sentences in \"+key+ \": \",len(df_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "previous-language",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (3.4.4)\n",
      "Requirement already satisfied: six in /home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from nltk) (1.15.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/ubuntu/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "single-standing",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data Augmentation\n",
    "\n",
    "import random\n",
    "import nltk\n",
    "from nltk.corpus import wordnet \n",
    "random.seed(50)\n",
    "\n",
    "#stop words list\n",
    "stop_words = ['i', 'me', 'my', 'myself', 'we', 'our', \n",
    "\t\t\t'ours', 'ourselves', 'you', 'your', 'yours', \n",
    "\t\t\t'yourself', 'yourselves', 'he', 'him', 'his', \n",
    "\t\t\t'himself', 'she', 'her', 'hers', 'herself', \n",
    "\t\t\t'it', 'its', 'itself', 'they', 'them', 'their', \n",
    "\t\t\t'theirs', 'themselves', 'what', 'which', 'who', \n",
    "\t\t\t'whom', 'this', 'that', 'these', 'those', 'am', \n",
    "\t\t\t'is', 'are', 'was', 'were', 'be', 'been', 'being', \n",
    "\t\t\t'have', 'has', 'had', 'having', 'do', 'does', 'did',\n",
    "\t\t\t'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or',\n",
    "\t\t\t'because', 'as', 'until', 'while', 'of', 'at', \n",
    "\t\t\t'by', 'for', 'with', 'about', 'against', 'between',\n",
    "\t\t\t'into', 'through', 'during', 'before', 'after', \n",
    "\t\t\t'above', 'below', 'to', 'from', 'up', 'down', 'in',\n",
    "\t\t\t'out', 'on', 'off', 'over', 'under', 'again', \n",
    "\t\t\t'further', 'then', 'once', 'here', 'there', 'when', \n",
    "\t\t\t'where', 'why', 'how', 'all', 'any', 'both', 'each', \n",
    "\t\t\t'few', 'more', 'most', 'other', 'some', 'such', 'no', \n",
    "\t\t\t'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', \n",
    "\t\t\t'very', 's', 't', 'can', 'will', 'just', 'don', \n",
    "\t\t\t'should', 'now', '']\n",
    "\n",
    "\n",
    "def random_deletion(words, p=0.3): #sentence: list of words/tokens from a sentence\n",
    "    if len(words) == 1: # return if single word\n",
    "        return words\n",
    "    remaining = list(filter(lambda x: random.uniform(0,1) > p,words)) \n",
    "    if len(remaining) == 0: # if not left, sample a random word\n",
    "        return [random.choice(words)] \n",
    "    else:\n",
    "        return remaining\n",
    "\n",
    "def random_swap(sentence, n=5): #sentence: list of words/tokens from a sentence\n",
    "    sen_len = len(sentence)\n",
    "    length = range(sen_len)\n",
    "    if (sen_len>1): \n",
    "        for _ in range(n):\n",
    "            idx1, idx2 = random.sample(length, 2)\n",
    "            sentence[idx1], sentence[idx2] = sentence[idx2], sentence[idx1] \n",
    "        return sentence\n",
    "    else:\n",
    "        return sentence\n",
    "\n",
    "\n",
    "def synonym_replacement(words, n=5):\n",
    "\tnew_words = words.copy()\n",
    "\trandom_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "\trandom.shuffle(random_word_list)\n",
    "\tnum_replaced = 0\n",
    "\tfor random_word in random_word_list:\n",
    "\t\tsynonyms = get_synonyms(random_word)\n",
    "\t\tif len(synonyms) >= 1:\n",
    "\t\t\tsynonym = random.choice(list(synonyms))\n",
    "\t\t\tnew_words = [synonym if word == random_word else word for word in new_words]\n",
    "\t\t\t#print(\"replaced\", random_word, \"with\", synonym)\n",
    "\t\t\tnum_replaced += 1\n",
    "\t\tif num_replaced >= n: #only replace up to n words\n",
    "\t\t\tbreak\n",
    "\n",
    "\t#this is stupid but we need it, trust me\n",
    "\tsentence = ' '.join(new_words)\n",
    "\tnew_words = sentence.split(' ')\n",
    "\n",
    "\treturn new_words\n",
    "\n",
    "def get_synonyms(word):\n",
    "\tsynonyms = set()\n",
    "\tfor syn in wordnet.synsets(word): \n",
    "\t\tfor l in syn.lemmas(): \n",
    "\t\t\tsynonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "\t\t\tsynonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "\t\t\tsynonyms.add(synonym) \n",
    "\tif word in synonyms:\n",
    "\t\tsynonyms.remove(word)\n",
    "\treturn list(synonyms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "musical-nursing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = pd.read_csv(\"dataset/train.csv\")\n",
    "df = shuffle(df)\n",
    "df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "#df_aug = pd.DataFrame(columns=[\"Sentence\",\"Label\",\"Dataset\"])\n",
    "#df_aug = df.astype(dtype={'Sentence':str,'Label':int,'Dataset':str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "spatial-scale",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    text = text.replace(\"`\",\"'\")\n",
    "    text = re.sub(r\"[-](LRB|RRB|RSB|LSB)[-]\",\"\",text)\n",
    "    text = re.sub(r\"[^0-9a-zA-Z',\\-\\.]\",\" \",text)\n",
    "    text = \" \".join([t for t in text.split(\" \") if t is not \"\"])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "honest-parent",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The Rock is destined to be the 21st Century 's new '' Conan '' and that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\""
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"\"\"The Rock is destined to be the 21st Century 's new `` Conan '' and \n",
    "           that he 's going to make a splash even greater than Arnold Schwarzenegger , Jean-Claud Van Damme or Steven Segal .\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "stuck-naples",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapper Functions\n",
    "def synonym_replace_aug(text):\n",
    "    return \" \".join(synonym_replacement(text.split(\" \"),n=4))\n",
    "def random_delete_aug(text):\n",
    "    return \" \".join(random_deletion(text.split(\" \")))\n",
    "def random_swap_aug(text):\n",
    "    return \" \".join(random_swap(text.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "facial-notebook",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken :  0.42549633979797363\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "start_time = time.time()\n",
    "\n",
    "df_sr = df[:2500].sample(1200).copy()\n",
    "df_sr[\"Sentence\"] = df_sr[\"Sentence\"].apply(clean_text).apply(synonym_replace_aug)\n",
    "\n",
    "df_rd = df[2501:4500].sample(900).copy()\n",
    "df_rd[\"Sentence\"] = df_rd[\"Sentence\"].apply(clean_text).apply(random_delete_aug)\n",
    "\n",
    "df_rs = df[4500:6000].sample(400).copy()\n",
    "df_rs[\"Sentence\"] = df_rs[\"Sentence\"].apply(clean_text).apply(random_swap_aug)\n",
    "\n",
    "print(\"Time taken : \", time.time() - start_time)\n",
    "df_aug =  pd.concat([df_sr,df_rd,df_rs])\n",
    "df_aug.to_csv(\"dataset/aug_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "connected-lying",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below may not mark Mr. Twohy 's emergence into the mainstream , but his promise remains undiminished .\n",
      "Below may not score Mr. Twohy 's outgrowth into the mainstream , but his hope continue undiminished .\n"
     ]
    }
   ],
   "source": [
    "#Example for SR\n",
    "print(df.loc[10,\"Sentence\"])\n",
    "print(df_sr.loc[10,\"Sentence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "uniform-lending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: google-trans-new in /Users/pavisj/pytorch/lib/python3.7/site-packages (1.1.9)\n"
     ]
    }
   ],
   "source": [
    "#Back Translate\n",
    "!pip install google-trans-new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "appointed-substitute",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[6000:].to_csv(\"dataset/back_trans_candidates.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-benchmark",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from google_trans_new import google_translator  \n",
    "\n",
    "avbl_langs = ['af', 'sq', 'am', 'ar', 'hy', 'az', 'eu', 'be', 'bn', 'bs', 'bg', 'ca', 'ceb', 'ny', 'zh-cn', 'zh-tw', 'co', 'hr', 'cs', 'da', 'nl', 'en', 'eo', 'et', 'tl', 'fi', 'fr', 'fy', 'gl', 'ka', 'de', 'el', 'gu', 'ht', 'ha', 'haw', 'iw', 'he', 'hi', 'hmn', 'hu', 'is', 'ig', 'id', 'ga', 'it', 'ja', 'jw', 'kn', 'kk', 'km', 'ko', 'ku', 'ky', 'lo', 'la', 'lv', 'lt', 'lb', 'mk', 'mg', 'ms', 'ml', 'mt', 'mi', 'mr', 'mn', 'my', 'ne', 'no', 'or', 'ps', 'fa', 'pl', 'pt', 'pa', 'ro', 'ru', 'sm', 'gd', 'sr', 'st', 'sn', 'sd', 'si', 'sk', 'sl', 'so', 'es', 'su', 'sw', 'sv', 'tg', 'ta', 'te', 'th', 'tr', 'uk', 'ur', 'ug', 'uz', 'vi', 'cy', 'xh', 'yi', 'yo', 'zu']\n",
    "\n",
    "def _apply_df(args):\n",
    "    df, func, num, kwargs = args\n",
    "    return num, df.apply(func, **kwargs)\n",
    "\n",
    "def apply_by_multiprocessing(df,func,**kwargs):\n",
    "    workers=kwargs.pop('workers')\n",
    "    pool = multiprocessing.Pool(processes=workers)\n",
    "    result = pool.map(_apply_df, [(d, func, i, kwargs) for i,d in enumerate(np.array_split(df, workers))])  \n",
    "    pool.close()\n",
    "    result=sorted(result,key=lambda x:x[0])\n",
    "    return pd.concat([i[1] for i in result])\n",
    "\n",
    "def square(row):\n",
    "    text =row[\"Sentence\"]\n",
    "    #print(\"Text: \", text)\n",
    "    \n",
    "    translator = google_translator(timeout=5) \n",
    "    trans_lang = random.choice(avbl_langs)  \n",
    "      \n",
    "    translate_text = translator.translate(\n",
    "      translator.translate(text,lang_tgt=trans_lang),\n",
    "      lang_src = trans_lang,\n",
    "      lang_tgt='en')\n",
    "    \n",
    "    row[\"Sentence\"] = translate_text\n",
    "    #print(\"Translated text : \", row[\"Sentence\"] )\n",
    "    return row\n",
    "\n",
    "df_bt = df[6000:7000].copy()\n",
    "start_time=time.time()\n",
    "df_bt = apply_by_multiprocessing(df_bt, square, axis=1, workers=5) \n",
    "print(\"Time Taken for 1000 sentences : \", time.time()-start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "neither-craps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6010</th>\n",
       "      <td>10547</td>\n",
       "      <td>Creepy but ultimately unsatisfying thriller .</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6011</th>\n",
       "      <td>4111</td>\n",
       "      <td>Though the opera itself takes place mostly ind...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6012</th>\n",
       "      <td>2027</td>\n",
       "      <td>Although devoid of objectivity and full of nos...</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6013</th>\n",
       "      <td>8749</td>\n",
       "      <td>Narc may not get an ` A ' for originality , bu...</td>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6014</th>\n",
       "      <td>1076</td>\n",
       "      <td>An intriguing look at the French film industry...</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                           Sentence  Label  \\\n",
       "6010       10547      Creepy but ultimately unsatisfying thriller .      1   \n",
       "6011        4111  Though the opera itself takes place mostly ind...      2   \n",
       "6012        2027  Although devoid of objectivity and full of nos...      3   \n",
       "6013        8749  Narc may not get an ` A ' for originality , bu...      3   \n",
       "6014        1076  An intriguing look at the French film industry...      3   \n",
       "\n",
       "     Dataset  \n",
       "6010    test  \n",
       "6011   train  \n",
       "6012   train  \n",
       "6013    test  \n",
       "6014   train  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[6010:6020].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "boolean-bidding",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Sentence</th>\n",
       "      <th>Label</th>\n",
       "      <th>Dataset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6010</th>\n",
       "      <td>10547</td>\n",
       "      <td>Creepy but eventually insufficient thrillers.</td>\n",
       "      <td>1</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6011</th>\n",
       "      <td>4111</td>\n",
       "      <td>Although the Opera itself is carried out the r...</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6012</th>\n",
       "      <td>2027</td>\n",
       "      <td>Although it is lacking objectivity and now is ...</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6013</th>\n",
       "      <td>8749</td>\n",
       "      <td>The narc may not get a `a 'for originality, bu...</td>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6014</th>\n",
       "      <td>1076</td>\n",
       "      <td>Looking amazing in the French film industry du...</td>\n",
       "      <td>3</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                                           Sentence  Label  \\\n",
       "6010       10547     Creepy but eventually insufficient thrillers.       1   \n",
       "6011        4111  Although the Opera itself is carried out the r...      2   \n",
       "6012        2027  Although it is lacking objectivity and now is ...      3   \n",
       "6013        8749  The narc may not get a `a 'for originality, bu...      3   \n",
       "6014        1076  Looking amazing in the French film industry du...      3   \n",
       "\n",
       "     Dataset  \n",
       "6010    test  \n",
       "6011   train  \n",
       "6012   train  \n",
       "6013    test  \n",
       "6014   train  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df_bt[10:20].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "lesser-accountability",
   "metadata": {},
   "outputs": [],
   "source": [
    "#tried but could not\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "downtown-preparation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.utils import shuffle\n",
    "df_train = pd.read_csv(\"dataset/train.csv\", index_col=0)\n",
    "df_aug = pd.read_csv(\"dataset/aug_df.csv\",index_col=0)\n",
    "df_bt = pd.read_csv(\"dataset/Back_Translation.csv\",index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "detailed-access",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_back_trans = pd.DataFrame(columns=df_train.columns)\n",
    "df_back_trans[\"Sentence\"]= df_bt[\"to_en\"]\n",
    "df_back_trans[\"Label\"]= df_bt[\"Label\"]\n",
    "df_back_trans[\"Dataset\"]= df_bt[\"Dataset\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "affiliated-catch",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13044\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df_augmented_dataset = pd.concat([df_train, df_aug,df_back_trans.sample(2000)])\n",
    "print(len(df_augmented_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "alpine-newcastle",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented_dataset = shuffle(df_augmented_dataset)\n",
    "df_augmented_dataset.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "reliable-fusion",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3    3545\n",
       "1    3396\n",
       "2    2452\n",
       "4    1985\n",
       "0    1666\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_augmented_dataset.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "thermal-butterfly",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_augmented_dataset.to_csv(\"dataset/train_aug.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "digital-slovakia",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"dataset/test.csv\")\n",
    "df_dev=pd.read_csv(\"dataset/dev.csv\")\n",
    "df_test_combined = pd.concat([df_test,df_dev])\n",
    "df_test_combined = shuffle(df_test_combined)\n",
    "df_test_combined.reset_index(inplace=True, drop=True)\n",
    "df_test_combined.to_csv(\"dataset/test_dev.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "loving-movie",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of records \n",
      "Train dataset : 13044 0.80\n",
      "Test Dataset: 3311 0.20\n",
      "Total : 16355\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#80/20 split \n",
    "train_n = len(df_augmented_dataset)\n",
    "test_n = len(df_test_combined)\n",
    "total = train_n + test_n\n",
    "percent_train = train_n/total\n",
    "percent_test = test_n/total\n",
    "print(f\"\"\"Number of records \\nTrain dataset : %d \n",
    "      %.2f\\nTest Dataset: %d %.2f\\nTotal : %d\"\"\"\n",
    "      %(train_n,percent_train,test_n,percent_test,total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "comic-distance",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    922\n",
       "3    789\n",
       "2    618\n",
       "4    564\n",
       "0    418\n",
       "Name: Label, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test_combined.Label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suburban-duplicate",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
