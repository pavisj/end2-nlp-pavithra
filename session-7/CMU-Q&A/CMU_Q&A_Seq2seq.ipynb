{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CMU Q&A Seq2seq.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeDLJzHFSbN5"
      },
      "source": [
        "## CMU QA Task Using Seq2Seq Architecture\n",
        "\n",
        "**Objective**: As part of Assignment 7 - Part 2, we will be using Q&A from the CMU Dataset to generate Answers based on the Questions from the Dataset. \n",
        "\n",
        "**Task**: For a given Question as input, using Seq2Seq generate an answer. We are using the model discussed in Session 7 of END2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZMrY8F2TT85"
      },
      "source": [
        "### 1. Downloading and loading the dataset into DataFrames\n",
        "\n",
        "#### About the dataset\n",
        "\n",
        "This dataset was curated by CMU and Pitt Students from the period from 2008 - 2010.\n",
        "\n",
        "There are three directories, corresponding to the yeats 2008, 2009 and 2010\n",
        "\n",
        "In each of these three directories, there is a file \"question_answer_pairs.txt\" which contains the questions and answers. The columns in the TSV file are as follows:\n",
        "\n",
        "**ArticleTitle**: Name of the Wikipedia article from which questions and answers initially came. <br>\n",
        "**Question:** The Question <br>\n",
        "**Answer :** The Answer <br>\n",
        "**DifficultyFromQuestioner:** Prescribed difficulty rating for the question as given to the question-writer. <br>\n",
        "**DifficultyFromAnswerer:** Difficulty rating assigned by the individual who evaluated and answered the question <br>\n",
        "**ArticleFile:** Relative path to the prefix of the article files. html files (.htm) and cleaned text (.txt) files are provided. <br>\n",
        "\n",
        "<br>\n",
        "\n",
        "There are frequently multiple lines with the same question, which appear if those questions were answered by multiple individuals. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d27S03zbAAPu"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "from torchtext.legacy.datasets import Multi30k\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy.data import Field, BucketIterator\n",
        "\n",
        "\n",
        "import spacy\n",
        "import numpy as np\n",
        "\n",
        "import random\n",
        "import math\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.expand_frame_repr', False)\n",
        "pd.set_option('display.max_colwidth', None)\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TUQncHSTLLdL"
      },
      "source": [
        "SEED = 1234\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "#!pip install spacy --upgrade"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PIkQ72_TQuYt",
        "outputId": "5eaf6a02-92c8-46e8-cbb3-670b98b66a3a"
      },
      "source": [
        "!wget http://www.cs.cmu.edu/~ark/QA-data/data/Question_Answer_Dataset_v1.2.tar.gz"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-06-24 20:07:30--  http://www.cs.cmu.edu/~ark/QA-data/data/Question_Answer_Dataset_v1.2.tar.gz\n",
            "Resolving www.cs.cmu.edu (www.cs.cmu.edu)... 128.2.42.95\n",
            "Connecting to www.cs.cmu.edu (www.cs.cmu.edu)|128.2.42.95|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8254496 (7.9M) [application/x-gzip]\n",
            "Saving to: ‘Question_Answer_Dataset_v1.2.tar.gz’\n",
            "\n",
            "Question_Answer_Dat 100%[===================>]   7.87M   381KB/s    in 14s     \n",
            "\n",
            "2021-06-24 20:07:44 (577 KB/s) - ‘Question_Answer_Dataset_v1.2.tar.gz’ saved [8254496/8254496]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqPD0bogQ4i5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ef148fc-15ec-47df-dbf4-a190d4f582cf"
      },
      "source": [
        "!tar -xvzf Question_Answer_Dataset_v1.2.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question_Answer_Dataset_v1.2/\n",
            "Question_Answer_Dataset_v1.2/S08/\n",
            "Question_Answer_Dataset_v1.2/S08/question_answer_pairs.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set4/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set3/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set2/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S08/data/set1/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/\n",
            "Question_Answer_Dataset_v1.2/S10/question_answer_pairs.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set6/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set4/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set3/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set2/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set5/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S10/data/set1/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/LICENSE-S08,S09\n",
            "Question_Answer_Dataset_v1.2/README.v1.2\n",
            "Question_Answer_Dataset_v1.2/S09/\n",
            "Question_Answer_Dataset_v1.2/S09/question_answer_pairs.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set4/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set3/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set2/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set5/a3o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a6.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a3.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a3.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a5.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a4o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a3.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a9.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a2.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a9.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a4.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a4.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a4.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a2.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a7o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a6.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a5o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a1o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a9o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a8.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a10.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a10.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a8.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/topics.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a2o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a8.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a7.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a1.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a2.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a10.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a10o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a6.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a7.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a9.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a6o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a8o.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a1.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a1.txt\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a7.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a5.txt.clean\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a5.htm\n",
            "Question_Answer_Dataset_v1.2/S09/data/set1/a3o.htm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-4D69QqSYDF",
        "outputId": "dcea15f3-5652-4144-ea6e-ad9fe5ae7ece"
      },
      "source": [
        "import pandas as pd\n",
        "df_08 = pd.read_csv(\"/content/Question_Answer_Dataset_v1.2/S08/question_answer_pairs.txt\", sep='\\t', engine=\"python\",error_bad_lines=False)\n",
        "df_09 = pd.read_csv(\"/content/Question_Answer_Dataset_v1.2/S09/question_answer_pairs.txt\", sep='\\t', engine=\"python\",error_bad_lines=False)\n",
        "df_10 = pd.read_csv(\"/content/Question_Answer_Dataset_v1.2/S10/question_answer_pairs.txt\", sep='\\t', engine=\"python\",error_bad_lines=False)\n",
        "df = pd.concat([df_08,df_09,df_10])\n",
        "\n",
        "\n",
        "df['Question'] = df['Question'].astype(str)\n",
        "df['Answer'] = df['Answer'].astype(str)\n",
        "df = df.dropna()\n",
        "df.to_csv(\"cmu_qa_dataset.csv\")\n",
        "\n",
        "print(\"Number of records : \",len(df))\n",
        "print(df[['Question','Answer']].head(n = 20))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of records :  2731\n",
            "                                                             Question                                                                      Answer\n",
            "0   Was Abraham Lincoln the sixteenth President of the United States?                                                                         yes\n",
            "1   Was Abraham Lincoln the sixteenth President of the United States?                                                                        Yes.\n",
            "2                  Did Lincoln sign the National Banking Act of 1863?                                                                         yes\n",
            "3                  Did Lincoln sign the National Banking Act of 1863?                                                                        Yes.\n",
            "4                                    Did his mother die of pneumonia?                                                                          no\n",
            "5                                    Did his mother die of pneumonia?                                                                         No.\n",
            "6                       How many long was Lincoln's formal education?                                                                   18 months\n",
            "7                       How many long was Lincoln's formal education?                                                                  18 months.\n",
            "8                        When did Lincoln begin his political career?                                                                        1832\n",
            "9                        When did Lincoln begin his political career?                                                                       1832.\n",
            "10                   What did The Legal Tender Act of 1862 establish?   the United States Note, the first paper currency in United States history\n",
            "11                   What did The Legal Tender Act of 1862 establish?  The United States Note, the first paper currency in United States history.\n",
            "12                                Who suggested Lincoln grow a beard?                                                    11-year-old Grace Bedell\n",
            "13                                Who suggested Lincoln grow a beard?                                                               Grace Bedell.\n",
            "14       When did the Gettysburg address argue that America was born?                                                                        1776\n",
            "15       When did the Gettysburg address argue that America was born?                                                                       1776.\n",
            "16        Did Lincoln beat John C. Breckinridge in the 1860 election?                                                                         yes\n",
            "17        Did Lincoln beat John C. Breckinridge in the 1860 election?                                                                        Yes.\n",
            "18      Was Abraham Lincoln the first President of the United States?                                                                          No\n",
            "19      Was Abraham Lincoln the first President of the United States?                                                                          No\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Skipping line 765: '\t' expected after '\"'\n",
            "Skipping line 876: '\t' expected after '\"'\n",
            "Skipping line 1219: '\t' expected after '\"'\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gGS-Zzs2Aglb",
        "outputId": "7069caab-08e1-4134-bf7f-95757cfc8fbb"
      },
      "source": [
        "%%bash\n",
        "python -m spacy download en\n",
        "#python -m spacy download de"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.7/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.7/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKK9oA7OArZK"
      },
      "source": [
        "#spacy_de = spacy.load('de_core_news_sm')\n",
        "spacy_en = spacy.load('en_core_web_sm')"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPubajj7A0pY"
      },
      "source": [
        "'''def tokenize_de(text):\n",
        "    \"\"\"\n",
        "    Tokenizes German text from a string into a list of strings (tokens) and reverses it\n",
        "    \"\"\"\n",
        "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]'''\n",
        "\n",
        "def tokenize_en(text):\n",
        "    \"\"\"\n",
        "    Tokenizes English text from a string into a list of strings (tokens)\n",
        "    \"\"\"\n",
        "    return [tok.text.lower() for tok in spacy_en.tokenizer(text)]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ztK5PjShBN_M"
      },
      "source": [
        "Question = Field(sequential=True,\n",
        "                 #batch_first = True,\n",
        "                 tokenize = tokenize_en, \n",
        "                 init_token = '<sos>', \n",
        "                 eos_token = '<eos>')\n",
        "\n",
        "\n",
        "Answer = Field(sequential=True,\n",
        "                 #batch_first = True,\n",
        "                 tokenize = tokenize_en, \n",
        "                 init_token = '<sos>', \n",
        "                 eos_token = '<eos>')\n",
        "\n",
        "fields = [('Question', Question),('Answer',Answer)]"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v3y3pb2vXe1f"
      },
      "source": [
        "example = [data.Example.fromlist([df.Question.iloc[i],df.Answer.iloc[i]], fields) for i in range(df.shape[0])] \n",
        "train_data, test_data = data.Dataset(example, fields).split(split_ratio = 0.7)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y3cCTrkoB7fu",
        "outputId": "53d0b868-a02d-4a19-d3e7-75f42efb9c75"
      },
      "source": [
        "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
        "#print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
        "print(f\"Number of testing examples: {len(test_data.examples)}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 1912\n",
            "Number of testing examples: 819\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN6IJggTCBH6",
        "outputId": "1bda0d50-4ac4-43b5-fe54-6f4cff67d4fb"
      },
      "source": [
        "print(vars(train_data.examples[0]))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Question': ['is', 'antwerp', 'in', 'belgium', '?'], 'Answer': ['yes']}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BY3qVJbpK_2L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f627cf4-c3c1-4459-f0e2-a9a51165d414"
      },
      "source": [
        "Question.build_vocab(train_data, min_freq = 2)\n",
        "Answer.build_vocab(train_data, min_freq = 2)\n",
        "\n",
        "print('Top 10 words in Question Vocab :', list(Question.vocab.freqs.most_common(10)))\n",
        "\n",
        "import os, pickle\n",
        "with open('CMU_Question_tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Question.vocab.stoi, tokens)\n",
        "\n",
        "  \n",
        "with open('CMU_Answer_tokenizer.pkl', 'wb') as tokens: \n",
        "    pickle.dump(Answer.vocab.stoi, tokens)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Top 10 words in Question Vocab : [('?', 1887), ('the', 999), ('of', 585), ('is', 565), ('what', 531), ('a', 371), ('in', 341), ('did', 284), ('was', 281), ('are', 253)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy0xQVgdLBkm",
        "outputId": "eaadac29-326a-4446-fb2e-7afb62184e10"
      },
      "source": [
        "print(f\"Unique tokens in source (Question) vocabulary: {len(Question.vocab)}\")\n",
        "print(f\"Unique tokens in target (Answer) vocabulary: {len(Answer.vocab)}\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in source (Question) vocabulary: 1661\n",
            "Unique tokens in target (Answer) vocabulary: 1181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4I_59ly4LC4T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e191249-1c49-4c8e-816d-10de592f4e3c"
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(\"Curent Device is \", device)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Curent Device is  cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_0GNSQSCLEOB"
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, test_data), \n",
        "    batch_size = BATCH_SIZE,\n",
        "    sort_key = lambda x: len(x.Question),\n",
        "    sort_within_batch=True,\n",
        "    device = device)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ddiTgU8hLFj1"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, src):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(src))\n",
        "        \n",
        "        #embedded = [src len, batch size, emb dim]\n",
        "        \n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        \n",
        "        #outputs = [src len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #outputs are always from the top hidden layer\n",
        "        \n",
        "        return hidden, cell"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKKhHPD2LHX1"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hid_dim, n_layers, dropout):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.output_dim = output_dim\n",
        "        self.hid_dim = hid_dim\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        \n",
        "        self.rnn = nn.LSTM(emb_dim, hid_dim, n_layers, dropout = dropout)\n",
        "        \n",
        "        self.fc_out = nn.Linear(hid_dim, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, input, hidden, cell):\n",
        "        \n",
        "        #input = [batch size]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #n directions in the decoder will both always be 1, therefore:\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #context = [n layers, batch size, hid dim]\n",
        "        \n",
        "        input = input.unsqueeze(0)\n",
        "        \n",
        "        #input = [1, batch size]\n",
        "        \n",
        "        embedded = self.dropout(self.embedding(input))\n",
        "        \n",
        "        #embedded = [1, batch size, emb dim]\n",
        "                \n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        \n",
        "        #output = [seq len, batch size, hid dim * n directions]\n",
        "        #hidden = [n layers * n directions, batch size, hid dim]\n",
        "        #cell = [n layers * n directions, batch size, hid dim]\n",
        "        \n",
        "        #seq len and n directions will always be 1 in the decoder, therefore:\n",
        "        #output = [1, batch size, hid dim]\n",
        "        #hidden = [n layers, batch size, hid dim]\n",
        "        #cell = [n layers, batch size, hid dim]\n",
        "        \n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        \n",
        "        #prediction = [batch size, output dim]\n",
        "        \n",
        "        return prediction, hidden, cell"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YIf8EPxVLJhx"
      },
      "source": [
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder, device):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.device = device\n",
        "        \n",
        "        assert encoder.hid_dim == decoder.hid_dim, \\\n",
        "            \"Hidden dimensions of encoder and decoder must be equal!\"\n",
        "        assert encoder.n_layers == decoder.n_layers, \\\n",
        "            \"Encoder and decoder must have equal number of layers!\"\n",
        "        \n",
        "    def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n",
        "        \n",
        "        #src = [src len, batch size]\n",
        "        #trg = [trg len, batch size]\n",
        "        #teacher_forcing_ratio is probability to use teacher forcing\n",
        "        #e.g. if teacher_forcing_ratio is 0.75 we use ground-truth inputs 75% of the time\n",
        "        \n",
        "        batch_size = trg.shape[1]\n",
        "        trg_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.output_dim\n",
        "        \n",
        "        #tensor to store decoder outputs\n",
        "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n",
        "        \n",
        "        #last hidden state of the encoder is used as the initial hidden state of the decoder\n",
        "        hidden, cell = self.encoder(src)\n",
        "        \n",
        "        #first input to the decoder is the <sos> tokens\n",
        "        input = trg[0,:]\n",
        "        \n",
        "        for t in range(1, trg_len):\n",
        "            \n",
        "            #insert input token embedding, previous hidden and previous cell states\n",
        "            #receive output tensor (predictions) and new hidden and cell states\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            \n",
        "            #place predictions in a tensor holding predictions for each token\n",
        "            outputs[t] = output\n",
        "            \n",
        "            #decide if we are going to use teacher forcing or not\n",
        "            teacher_force = random.random() < teacher_forcing_ratio\n",
        "            \n",
        "            #get the highest predicted token from our predictions\n",
        "            top1 = output.argmax(1) \n",
        "            \n",
        "            #if teacher forcing, use actual next token as next input\n",
        "            #if not, use predicted token\n",
        "            input = trg[t] if teacher_force else top1\n",
        "        \n",
        "        return outputs"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3i0_86JVLL-9"
      },
      "source": [
        "INPUT_DIM = len(Question.vocab)\n",
        "OUTPUT_DIM = len(Answer.vocab)\n",
        "ENC_EMB_DIM = 256\n",
        "DEC_EMB_DIM = 256\n",
        "HID_DIM = 512\n",
        "N_LAYERS = 2\n",
        "ENC_DROPOUT = 0.5\n",
        "DEC_DROPOUT = 0.5\n",
        "\n",
        "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
        "dec = Decoder(OUTPUT_DIM, DEC_EMB_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
        "\n",
        "model = Seq2Seq(enc, dec, device).to(device)"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBhX5dKuLNar",
        "outputId": "40b43bc5-2ba2-4f66-b4da-2f60a4712556"
      },
      "source": [
        "def init_weights(m):\n",
        "    for name, param in m.named_parameters():\n",
        "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
        "        \n",
        "model.apply(init_weights)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Seq2Seq(\n",
              "  (encoder): Encoder(\n",
              "    (embedding): Embedding(1661, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              "  (decoder): Decoder(\n",
              "    (embedding): Embedding(1181, 256)\n",
              "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
              "    (fc_out): Linear(in_features=512, out_features=1181, bias=True)\n",
              "    (dropout): Dropout(p=0.5, inplace=False)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PSdevJJNLPOt",
        "outputId": "2d00f8c2-13b8-4c7a-c69e-39099c959cb3"
      },
      "source": [
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "\n",
        "print(f'The model has {count_parameters(model):,} trainable parameters')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The model has 8,689,821 trainable parameters\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-CvhZwYLQoT"
      },
      "source": [
        "optimizer = optim.Adam(model.parameters(),lr=0.0005)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kO11j3WELR5P"
      },
      "source": [
        "Answer_PAD_IDX = Answer.vocab.stoi[Answer.pad_token]\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index = Answer_PAD_IDX)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-HReR1sLS8w"
      },
      "source": [
        "def train(model, iterator, optimizer, criterion, clip):\n",
        "    \n",
        "    model.train()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    for i, batch in enumerate(iterator):\n",
        "        \n",
        "        ques = batch.Question\n",
        "        ans = batch.Answer\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        output = model(ques, ans)\n",
        "        \n",
        "        #ans = [ans len, batch size]\n",
        "        #output = [ans len, batch size, output dim]\n",
        "        \n",
        "        output_dim = output.shape[-1]\n",
        "        \n",
        "        output = output[1:].view(-1, output_dim)\n",
        "        ans = ans[1:].view(-1)\n",
        "        \n",
        "        #ans = [(ans len - 1) * batch size]\n",
        "        #output = [(ans len - 1) * batch size, output dim]\n",
        "        \n",
        "        loss = criterion(output, ans)\n",
        "        \n",
        "        loss.backward()\n",
        "        \n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "        \n",
        "        optimizer.step()\n",
        "        \n",
        "        epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfm7iiOmLUhv"
      },
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    \n",
        "    with torch.no_grad():\n",
        "    \n",
        "        for i, batch in enumerate(iterator):\n",
        "\n",
        "            ques = batch.Question\n",
        "            ans = batch.Answer\n",
        "\n",
        "            output = model(ques, ans, 0) #turn off teacher forcing\n",
        "\n",
        "            #ans = [ans len, batch size]\n",
        "            #output = [ans len, batch size, output dim]\n",
        "\n",
        "            output_dim = output.shape[-1]\n",
        "            \n",
        "            output = output[1:].view(-1, output_dim)\n",
        "            ans = ans[1:].view(-1)\n",
        "\n",
        "            #trg = [(trg len - 1) * batch size]\n",
        "            #output = [(trg len - 1) * batch size, output dim]\n",
        "\n",
        "            loss = criterion(output, ans)\n",
        "            \n",
        "            epoch_loss += loss.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXrLres2LWHg"
      },
      "source": [
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HGdzElxhLXKm",
        "outputId": "8a92ac10-45e4-497e-9448-de884e7ad880"
      },
      "source": [
        "N_EPOCHS = 35\n",
        "CLIP = 1\n",
        "\n",
        "best_test_loss = float('inf')\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    test_loss = evaluate(model, test_iterator, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "    \n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if test_loss < best_test_loss:\n",
        "        best_test_loss = test_loss\n",
        "        torch.save(model.state_dict(), 'cmu-qa-model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Test. Loss: {test_loss:.3f} |  Test. PPL: {math.exp(test_loss):7.3f}')"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch: 01 | Time: 0m 1s\n",
            "\tTrain Loss: 5.900 | Train PPL: 364.953\n",
            "\t Test. Loss: 4.020 |  Test. PPL:  55.697\n",
            "Epoch: 02 | Time: 0m 1s\n",
            "\tTrain Loss: 4.622 | Train PPL: 101.747\n",
            "\t Test. Loss: 3.802 |  Test. PPL:  44.803\n",
            "Epoch: 03 | Time: 0m 1s\n",
            "\tTrain Loss: 4.517 | Train PPL:  91.536\n",
            "\t Test. Loss: 3.803 |  Test. PPL:  44.825\n",
            "Epoch: 04 | Time: 0m 1s\n",
            "\tTrain Loss: 4.460 | Train PPL:  86.515\n",
            "\t Test. Loss: 3.774 |  Test. PPL:  43.556\n",
            "Epoch: 05 | Time: 0m 1s\n",
            "\tTrain Loss: 4.427 | Train PPL:  83.698\n",
            "\t Test. Loss: 3.724 |  Test. PPL:  41.437\n",
            "Epoch: 06 | Time: 0m 1s\n",
            "\tTrain Loss: 4.399 | Train PPL:  81.329\n",
            "\t Test. Loss: 3.753 |  Test. PPL:  42.638\n",
            "Epoch: 07 | Time: 0m 1s\n",
            "\tTrain Loss: 4.364 | Train PPL:  78.581\n",
            "\t Test. Loss: 3.789 |  Test. PPL:  44.205\n",
            "Epoch: 08 | Time: 0m 1s\n",
            "\tTrain Loss: 4.342 | Train PPL:  76.849\n",
            "\t Test. Loss: 3.746 |  Test. PPL:  42.372\n",
            "Epoch: 09 | Time: 0m 1s\n",
            "\tTrain Loss: 4.305 | Train PPL:  74.097\n",
            "\t Test. Loss: 3.762 |  Test. PPL:  43.033\n",
            "Epoch: 10 | Time: 0m 1s\n",
            "\tTrain Loss: 4.281 | Train PPL:  72.291\n",
            "\t Test. Loss: 3.746 |  Test. PPL:  42.346\n",
            "Epoch: 11 | Time: 0m 1s\n",
            "\tTrain Loss: 4.252 | Train PPL:  70.230\n",
            "\t Test. Loss: 3.772 |  Test. PPL:  43.467\n",
            "Epoch: 12 | Time: 0m 1s\n",
            "\tTrain Loss: 4.205 | Train PPL:  67.046\n",
            "\t Test. Loss: 3.704 |  Test. PPL:  40.596\n",
            "Epoch: 13 | Time: 0m 1s\n",
            "\tTrain Loss: 4.184 | Train PPL:  65.646\n",
            "\t Test. Loss: 3.741 |  Test. PPL:  42.159\n",
            "Epoch: 14 | Time: 0m 1s\n",
            "\tTrain Loss: 4.162 | Train PPL:  64.192\n",
            "\t Test. Loss: 3.752 |  Test. PPL:  42.591\n",
            "Epoch: 15 | Time: 0m 1s\n",
            "\tTrain Loss: 4.084 | Train PPL:  59.385\n",
            "\t Test. Loss: 3.735 |  Test. PPL:  41.869\n",
            "Epoch: 16 | Time: 0m 1s\n",
            "\tTrain Loss: 4.055 | Train PPL:  57.706\n",
            "\t Test. Loss: 3.713 |  Test. PPL:  40.967\n",
            "Epoch: 17 | Time: 0m 1s\n",
            "\tTrain Loss: 4.005 | Train PPL:  54.885\n",
            "\t Test. Loss: 3.690 |  Test. PPL:  40.029\n",
            "Epoch: 18 | Time: 0m 1s\n",
            "\tTrain Loss: 3.984 | Train PPL:  53.730\n",
            "\t Test. Loss: 3.651 |  Test. PPL:  38.525\n",
            "Epoch: 19 | Time: 0m 1s\n",
            "\tTrain Loss: 3.940 | Train PPL:  51.427\n",
            "\t Test. Loss: 3.730 |  Test. PPL:  41.668\n",
            "Epoch: 20 | Time: 0m 1s\n",
            "\tTrain Loss: 3.898 | Train PPL:  49.304\n",
            "\t Test. Loss: 3.702 |  Test. PPL:  40.531\n",
            "Epoch: 21 | Time: 0m 1s\n",
            "\tTrain Loss: 3.876 | Train PPL:  48.254\n",
            "\t Test. Loss: 3.611 |  Test. PPL:  37.009\n",
            "Epoch: 22 | Time: 0m 1s\n",
            "\tTrain Loss: 3.831 | Train PPL:  46.103\n",
            "\t Test. Loss: 3.650 |  Test. PPL:  38.489\n",
            "Epoch: 23 | Time: 0m 1s\n",
            "\tTrain Loss: 3.778 | Train PPL:  43.735\n",
            "\t Test. Loss: 3.677 |  Test. PPL:  39.529\n",
            "Epoch: 24 | Time: 0m 1s\n",
            "\tTrain Loss: 3.799 | Train PPL:  44.644\n",
            "\t Test. Loss: 3.628 |  Test. PPL:  37.649\n",
            "Epoch: 25 | Time: 0m 1s\n",
            "\tTrain Loss: 3.749 | Train PPL:  42.464\n",
            "\t Test. Loss: 3.658 |  Test. PPL:  38.800\n",
            "Epoch: 26 | Time: 0m 1s\n",
            "\tTrain Loss: 3.718 | Train PPL:  41.179\n",
            "\t Test. Loss: 3.701 |  Test. PPL:  40.507\n",
            "Epoch: 27 | Time: 0m 1s\n",
            "\tTrain Loss: 3.704 | Train PPL:  40.594\n",
            "\t Test. Loss: 3.666 |  Test. PPL:  39.107\n",
            "Epoch: 28 | Time: 0m 1s\n",
            "\tTrain Loss: 3.656 | Train PPL:  38.718\n",
            "\t Test. Loss: 3.672 |  Test. PPL:  39.345\n",
            "Epoch: 29 | Time: 0m 1s\n",
            "\tTrain Loss: 3.630 | Train PPL:  37.723\n",
            "\t Test. Loss: 3.663 |  Test. PPL:  38.962\n",
            "Epoch: 30 | Time: 0m 1s\n",
            "\tTrain Loss: 3.608 | Train PPL:  36.896\n",
            "\t Test. Loss: 3.683 |  Test. PPL:  39.747\n",
            "Epoch: 31 | Time: 0m 1s\n",
            "\tTrain Loss: 3.567 | Train PPL:  35.394\n",
            "\t Test. Loss: 3.660 |  Test. PPL:  38.850\n",
            "Epoch: 32 | Time: 0m 1s\n",
            "\tTrain Loss: 3.552 | Train PPL:  34.885\n",
            "\t Test. Loss: 3.676 |  Test. PPL:  39.494\n",
            "Epoch: 33 | Time: 0m 1s\n",
            "\tTrain Loss: 3.463 | Train PPL:  31.912\n",
            "\t Test. Loss: 3.620 |  Test. PPL:  37.320\n",
            "Epoch: 34 | Time: 0m 1s\n",
            "\tTrain Loss: 3.460 | Train PPL:  31.830\n",
            "\t Test. Loss: 3.603 |  Test. PPL:  36.716\n",
            "Epoch: 35 | Time: 0m 1s\n",
            "\tTrain Loss: 3.444 | Train PPL:  31.301\n",
            "\t Test. Loss: 3.649 |  Test. PPL:  38.443\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OH-9wcDtLYi5",
        "outputId": "687f1f0a-b373-4f2b-88df-8708d0a98934"
      },
      "source": [
        "model.load_state_dict(torch.load('cmu-qa-model.pt'))\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "| Test Loss: 3.603 | Test PPL:  36.716 |\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjUCB1yYMfIa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73b54552-5050-4be6-ff9b-349d2f781389"
      },
      "source": [
        "id = 2\n",
        "\n",
        "example = train_data.examples[id]\n",
        "print('Question: ', ' '.join(example.Question))\n",
        "print('Answer: ', ' '.join(example.Answer))"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Question:  did lincoln win the election of 1860 ?\n",
            "Answer:  yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "p22ZnspNLBRo",
        "outputId": "5008c177-63d8-4b48-c79e-4e7090f5e1e0"
      },
      "source": [
        "ques_tensor = Question.process([example.Question]).to(device)\n",
        "ans_tensor = Answer.process([example.Answer]).to(device)\n",
        "print(ans_tensor.shape)\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model(ques_tensor, ans_tensor, teacher_forcing_ratio=0)\n",
        "\n",
        "print(outputs.shape)\n",
        "output_idx = outputs[1:].squeeze(1).argmax(1)\n",
        "' '.join([Answer.vocab.itos[idx] for idx in output_idx])"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([3, 1])\n",
            "torch.Size([3, 1, 1181])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'yes <eos>'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RyBUqJQwLjKa",
        "outputId": "b6c689d9-e99a-41ad-dff9-1e7a58a3d0ba"
      },
      "source": [
        "#EOS needs to be considered as a special symbol. This is a bug which needs to be fixed\n",
        "\n",
        "import random\n",
        "n =  len(train_data.examples)\n",
        "for i in range(0,5): #number of tries is 5\n",
        "    id = random.randint(1,n)\n",
        "    example = train_data.examples[id]\n",
        "    a = example.Answer[::-1]\n",
        "    \n",
        "    q_tensor = Question.process([example.Question]).to(device)\n",
        "    a_tensor = Answer.process([example.Answer]).to(device)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(q_tensor, a_tensor, teacher_forcing_ratio=0)\n",
        "\n",
        "    output_idx = outputs[1:].squeeze(1).argmax(1)\n",
        "    \n",
        "    print('\\nInput Question: ', ' '.join(example.Question))\n",
        "    print('Target Output: ', \" \".join(a))\n",
        "    print('Generated Question: ', ' '.join([Answer.vocab.itos[idx] for idx in output_idx][::-1]))\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Input Question:  does the de young museum house the asian art museum\n",
            "Target Output:  yes\n",
            "Generated Question:  <eos> yes\n",
            "\n",
            "Input Question:  what do we refer musicians who play flute ?\n",
            "Target Output:  . flutist a or flautist a , player flute a\n",
            "Generated Question:  <eos> <eos> <eos> <eos> <eos> <eos> <unk> the of <unk> the\n",
            "\n",
            "Input Question:  where was commodore matthew c. perry sent to open japan to western trade ?\n",
            "Target Output:  japan to send perry c. matthew commodore\n",
            "Generated Question:  <eos> <eos> <unk> <unk> <unk> <unk> <unk> <unk>\n",
            "\n",
            "Input Question:  what unrelated water birds are ducks sometimes confused with ?\n",
            "Target Output:  coots and , gallinules , grebes , divers or loons\n",
            "Generated Question:  <unk> , <eos> <eos> <unk> , <unk> , <unk> <unk> <unk>\n",
            "\n",
            "Input Question:  do nearly all qataris profess islam ?\n",
            "Target Output:  yes\n",
            "Generated Question:  <eos> yes\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FoqEu1DRM91P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}